{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√©-processamento dos Datasets para Experimento RCBD\n",
    "\n",
    "**Equipe F**: Bernardo Bacha de Resende, Gustavo Augusto Faria dos Reis, Mar√≠lia Mac√™do de Melo\n",
    "\n",
    "**Disciplina**: EEE933 - Planejamento e An√°lise de Experimentos (2025/2)\n",
    "\n",
    "---\n",
    "\n",
    "Este notebook processa os 5 datasets de classifica√ß√£o bin√°ria, preparando-os para uso no experimento RCBD.\n",
    "\n",
    "**Datasets:**\n",
    "1. Breast Cancer (569 amostras)\n",
    "2. Titanic (891 amostras)\n",
    "3. Water Potability (3,276 amostras)\n",
    "4. Employee Attrition (4,653 amostras)\n",
    "5. Australia Rain (145,460 amostras ‚Üí amostragem para ~10k)\n",
    "\n",
    "**Pr√©-processamento aplicado:**\n",
    "- Remo√ß√£o de colunas n√£o informativas (IDs, nomes, etc.)\n",
    "- Tratamento de valores nulos (mediana para num√©rico, moda para categ√≥rico)\n",
    "- One-hot encoding para features categ√≥ricas\n",
    "- StandardScaler (z-score) para normaliza√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports e Configura√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Bibliotecas importadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì scikit-optimize j√° instalado\n"
     ]
    }
   ],
   "source": [
    "# Instalar scikit-optimize se necess√°rio\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import skopt\n",
    "    print(\"‚úì scikit-optimize j√° instalado\")\n",
    "except ImportError:\n",
    "    print(\"Instalando scikit-optimize...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-optimize\"])\n",
    "    print(\"‚úì scikit-optimize instalado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset 1: Breast Cancer\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- 569 amostras √ó 32 colunas\n",
    "- Target: `diagnosis` (M=Malignant, B=Benign)\n",
    "- Sem valores nulos\n",
    "- Dataset mais limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET 1: BREAST CANCER\n",
      "================================================================================\n",
      "Shape original: (569, 32)\n",
      "Colunas: ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n",
      "\n",
      "Valores nulos: 0\n",
      "\n",
      "‚úì Pr√©-processamento conclu√≠do!\n",
      "X_breast_cancer shape: (569, 30)\n",
      "y_breast_cancer shape: (569,)\n",
      "Distribui√ß√£o do target:\n",
      "diagnosis\n",
      "0    357\n",
      "1    212\n",
      "Name: count, dtype: int64\n",
      "Propor√ß√£o: diagnosis\n",
      "0    0.627417\n",
      "1    0.372583\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATASET 1: BREAST CANCER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar dataset\n",
    "df_breast = pd.read_csv('../data/breast_cancer.csv')\n",
    "print(f\"Shape original: {df_breast.shape}\")\n",
    "print(f\"Colunas: {list(df_breast.columns)}\")\n",
    "\n",
    "# Remover coluna ID\n",
    "df_breast = df_breast.drop(columns=['id'])\n",
    "\n",
    "# Separar target\n",
    "y_breast_cancer = df_breast['diagnosis'].map({'M': 1, 'B': 0})\n",
    "X_breast_cancer = df_breast.drop(columns=['diagnosis'])\n",
    "\n",
    "# Verificar nulos\n",
    "print(f\"\\nValores nulos: {X_breast_cancer.isnull().sum().sum()}\")\n",
    "\n",
    "# Normalizar com StandardScaler\n",
    "scaler_breast = StandardScaler()\n",
    "X_breast_cancer = pd.DataFrame(\n",
    "    scaler_breast.fit_transform(X_breast_cancer),\n",
    "    columns=X_breast_cancer.columns\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Pr√©-processamento conclu√≠do!\")\n",
    "print(f\"X_breast_cancer shape: {X_breast_cancer.shape}\")\n",
    "print(f\"y_breast_cancer shape: {y_breast_cancer.shape}\")\n",
    "print(f\"Distribui√ß√£o do target:\\n{y_breast_cancer.value_counts()}\")\n",
    "print(f\"Propor√ß√£o: {y_breast_cancer.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset 2: Titanic\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- 891 amostras √ó 12 colunas\n",
    "- Target: `Survived` (0/1)\n",
    "- Valores nulos em Age, Cabin, Embarked\n",
    "- Features categ√≥ricas: Sex, Embarked, Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET 2: TITANIC\n",
      "================================================================================\n",
      "Shape original: (891, 12)\n",
      "\n",
      "Valores nulos por coluna:\n",
      "Age         177\n",
      "Cabin       687\n",
      "Embarked      2\n",
      "dtype: int64\n",
      "\n",
      "Valores nulos ap√≥s tratamento: 0\n",
      "\n",
      "Colunas ap√≥s one-hot encoding (10): ['Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Embarked_Q', 'Embarked_S', 'Pclass_1', 'Pclass_2', 'Pclass_3']\n",
      "\n",
      "‚úì Pr√©-processamento conclu√≠do!\n",
      "X_titanic shape: (891, 10)\n",
      "y_titanic shape: (891,)\n",
      "Distribui√ß√£o do target:\n",
      "Survived\n",
      "0    549\n",
      "1    342\n",
      "Name: count, dtype: int64\n",
      "Propor√ß√£o: Survived\n",
      "0    0.616162\n",
      "1    0.383838\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATASET 2: TITANIC\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar dataset\n",
    "df_titanic = pd.read_csv('../data/titanic.csv')\n",
    "print(f\"Shape original: {df_titanic.shape}\")\n",
    "print(f\"\\nValores nulos por coluna:\\n{df_titanic.isnull().sum()[df_titanic.isnull().sum() > 0]}\")\n",
    "\n",
    "# Remover colunas n√£o informativas\n",
    "df_titanic = df_titanic.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\n",
    "\n",
    "# Separar target\n",
    "y_titanic = df_titanic['Survived']\n",
    "X_titanic = df_titanic.drop(columns=['Survived'])\n",
    "\n",
    "# Tratar valores nulos\n",
    "# Age: preencher com mediana\n",
    "X_titanic['Age'].fillna(X_titanic['Age'].median(), inplace=True)\n",
    "# Embarked: preencher com moda\n",
    "X_titanic['Embarked'].fillna(X_titanic['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "print(f\"\\nValores nulos ap√≥s tratamento: {X_titanic.isnull().sum().sum()}\")\n",
    "\n",
    "# One-hot encoding para categ√≥ricas\n",
    "categorical_cols = ['Sex', 'Embarked']\n",
    "X_titanic = pd.get_dummies(X_titanic, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "\n",
    "# Converter Pclass para dummy se ainda n√£o for num√©rico adequado\n",
    "# Pclass j√° √© num√©rico (1, 2, 3), mas pode fazer one-hot se preferir\n",
    "X_titanic = pd.get_dummies(X_titanic, columns=['Pclass'], prefix='Pclass', dtype=int)\n",
    "\n",
    "print(f\"\\nColunas ap√≥s one-hot encoding ({len(X_titanic.columns)}): {list(X_titanic.columns)}\")\n",
    "\n",
    "# Normalizar com StandardScaler\n",
    "scaler_titanic = StandardScaler()\n",
    "X_titanic = pd.DataFrame(\n",
    "    scaler_titanic.fit_transform(X_titanic),\n",
    "    columns=X_titanic.columns\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Pr√©-processamento conclu√≠do!\")\n",
    "print(f\"X_titanic shape: {X_titanic.shape}\")\n",
    "print(f\"y_titanic shape: {y_titanic.shape}\")\n",
    "print(f\"Distribui√ß√£o do target:\\n{y_titanic.value_counts()}\")\n",
    "print(f\"Propor√ß√£o: {y_titanic.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset 3: Water Potability\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- 3,276 amostras √ó 10 colunas\n",
    "- Target: `Potability` (0/1)\n",
    "- Valores nulos em pH, Sulfate, Trihalomethanes\n",
    "- Todas features num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET 3: WATER POTABILITY\n",
      "================================================================================\n",
      "Shape original: (3276, 10)\n",
      "\n",
      "Valores nulos por coluna:\n",
      "ph                 491\n",
      "Sulfate            781\n",
      "Trihalomethanes    162\n",
      "dtype: int64\n",
      "Preenchidos ph com mediana: 7.04\n",
      "Preenchidos Sulfate com mediana: 333.07\n",
      "Preenchidos Trihalomethanes com mediana: 66.62\n",
      "\n",
      "Valores nulos ap√≥s tratamento: 0\n",
      "\n",
      "‚úì Pr√©-processamento conclu√≠do!\n",
      "X_water_potability shape: (3276, 9)\n",
      "y_water_potability shape: (3276,)\n",
      "Distribui√ß√£o do target:\n",
      "Potability\n",
      "0    1998\n",
      "1    1278\n",
      "Name: count, dtype: int64\n",
      "Propor√ß√£o: Potability\n",
      "0    0.60989\n",
      "1    0.39011\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATASET 3: WATER POTABILITY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar dataset\n",
    "df_water = pd.read_csv('../data/water_potability.csv')\n",
    "print(f\"Shape original: {df_water.shape}\")\n",
    "print(f\"\\nValores nulos por coluna:\\n{df_water.isnull().sum()[df_water.isnull().sum() > 0]}\")\n",
    "\n",
    "# Separar target\n",
    "y_water_potability = df_water['Potability']\n",
    "X_water_potability = df_water.drop(columns=['Potability'])\n",
    "\n",
    "# Tratar valores nulos (preencher com mediana)\n",
    "for col in X_water_potability.columns:\n",
    "    if X_water_potability[col].isnull().sum() > 0:\n",
    "        X_water_potability[col].fillna(X_water_potability[col].median(), inplace=True)\n",
    "        print(f\"Preenchidos {col} com mediana: {X_water_potability[col].median():.2f}\")\n",
    "\n",
    "print(f\"\\nValores nulos ap√≥s tratamento: {X_water_potability.isnull().sum().sum()}\")\n",
    "\n",
    "# Normalizar com StandardScaler\n",
    "scaler_water = StandardScaler()\n",
    "X_water_potability = pd.DataFrame(\n",
    "    scaler_water.fit_transform(X_water_potability),\n",
    "    columns=X_water_potability.columns\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Pr√©-processamento conclu√≠do!\")\n",
    "print(f\"X_water_potability shape: {X_water_potability.shape}\")\n",
    "print(f\"y_water_potability shape: {y_water_potability.shape}\")\n",
    "print(f\"Distribui√ß√£o do target:\\n{y_water_potability.value_counts()}\")\n",
    "print(f\"Propor√ß√£o: {y_water_potability.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset 4: Employee Attrition\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- 4,653 amostras √ó 9 colunas\n",
    "- Target: `LeaveOrNot` (0=Ficou, 1=Saiu do emprego)\n",
    "- Sem valores nulos\n",
    "- Features categ√≥ricas: Education, City, Gender, EverBenched\n",
    "- **Aten√ß√£o**: Classes razoavelmente balanceadas (~34% sa√≠das)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET 4: EMPLOYEE ATTRITION\n",
      "================================================================================\n",
      "Shape original: (4653, 9)\n",
      "\n",
      "Valores nulos por coluna:\n",
      "0 (nenhum!)\n",
      "\n",
      "Valores nulos: 0\n",
      "\n",
      "Colunas categ√≥ricas para encoding: ['Education', 'City', 'Gender', 'EverBenched']\n",
      "\n",
      "Colunas ap√≥s one-hot encoding (10): ['JoiningYear', 'PaymentTier', 'Age', 'ExperienceInCurrentDomain', 'Education_Masters', 'Education_PHD', 'City_New Delhi', 'City_Pune', 'Gender_Male', 'EverBenched_Yes']\n",
      "\n",
      "‚úì Pr√©-processamento conclu√≠do!\n",
      "X_employee shape: (4653, 10)\n",
      "y_employee shape: (4653,)\n",
      "Distribui√ß√£o do target:\n",
      "LeaveOrNot\n",
      "0    3053\n",
      "1    1600\n",
      "Name: count, dtype: int64\n",
      "Propor√ß√£o: LeaveOrNot\n",
      "0    0.656136\n",
      "1    0.343864\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATASET 4: EMPLOYEE ATTRITION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar dataset\n",
    "df_employee = pd.read_csv('../data/Employee.csv')\n",
    "print(f\"Shape original: {df_employee.shape}\")\n",
    "print(f\"\\nValores nulos por coluna:\\n{df_employee.isnull().sum().sum()} (nenhum!)\")\n",
    "\n",
    "# Separar target\n",
    "y_employee = df_employee['LeaveOrNot']\n",
    "X_employee = df_employee.drop(columns=['LeaveOrNot'])\n",
    "\n",
    "print(f\"\\nValores nulos: {X_employee.isnull().sum().sum()}\")\n",
    "\n",
    "# Identificar e fazer one-hot encoding para categ√≥ricas\n",
    "categorical_cols = ['Education', 'City', 'Gender', 'EverBenched']\n",
    "print(f\"\\nColunas categ√≥ricas para encoding: {categorical_cols}\")\n",
    "\n",
    "X_employee = pd.get_dummies(X_employee, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "\n",
    "print(f\"\\nColunas ap√≥s one-hot encoding ({len(X_employee.columns)}): {list(X_employee.columns)}\")\n",
    "\n",
    "# Normalizar com StandardScaler\n",
    "scaler_employee = StandardScaler()\n",
    "X_employee = pd.DataFrame(\n",
    "    scaler_employee.fit_transform(X_employee),\n",
    "    columns=X_employee.columns\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Pr√©-processamento conclu√≠do!\")\n",
    "print(f\"X_employee shape: {X_employee.shape}\")\n",
    "print(f\"y_employee shape: {y_employee.shape}\")\n",
    "print(f\"Distribui√ß√£o do target:\\n{y_employee.value_counts()}\")\n",
    "print(f\"Propor√ß√£o: {y_employee.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset 5: Australia Rain (Weather)\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- 145,460 amostras √ó 23 colunas (MUITO GRANDE)\n",
    "- Target: `RainTomorrow` (Yes/No)\n",
    "- MUITOS valores nulos (~40% das features)\n",
    "- Features categ√≥ricas: WindGustDir, WindDir9am, WindDir3pm, RainToday\n",
    "\n",
    "**Estrat√©gia:**\n",
    "1. Remover Date e Location (n√£o informativas/muitas categorias)\n",
    "2. Remover linhas com muitos nulos (dropna)\n",
    "3. Fazer amostragem estratificada para ~10k amostras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET 5: AUSTRALIA RAIN (WEATHER)\n",
      "================================================================================\n",
      "Shape original: (145460, 23)\n",
      "\n",
      "Valores nulos por coluna:\n",
      "MinTemp           1485\n",
      "MaxTemp           1261\n",
      "Rainfall          3261\n",
      "Evaporation      62790\n",
      "Sunshine         69835\n",
      "WindGustDir      10326\n",
      "WindGustSpeed    10263\n",
      "WindDir9am       10566\n",
      "WindDir3pm        4228\n",
      "WindSpeed9am      1767\n",
      "WindSpeed3pm      3062\n",
      "Humidity9am       2654\n",
      "Humidity3pm       4507\n",
      "Pressure9am      15065\n",
      "Pressure3pm      15028\n",
      "Cloud9am         55888\n",
      "Cloud3pm         59358\n",
      "Temp9am           1767\n",
      "Temp3pm           3609\n",
      "RainToday         3261\n",
      "RainTomorrow      3267\n",
      "dtype: int64\n",
      "\n",
      "Total de colunas com nulos: 21\n",
      "\n",
      "Shape ap√≥s remo√ß√£o de Date e Location: (145460, 21)\n",
      "Shape ap√≥s remover target nulo: (142193, 21)\n",
      "\n",
      "Linhas removidas por nulos: 85773\n",
      "Shape ap√≥s dropna: X=(56420, 20), y=(56420,)\n",
      "\n",
      "Colunas categ√≥ricas identificadas: ['WindGustDir', 'WindDir9am', 'WindDir3pm']\n",
      "Colunas ap√≥s one-hot encoding: 62\n",
      "\n",
      "Fazendo amostragem estratificada de 56420 para 10000 amostras...\n",
      "Shape ap√≥s amostragem: X=(10000, 62), y=(10000,)\n",
      "\n",
      "‚úì Pr√©-processamento conclu√≠do!\n",
      "X_weather shape: (10000, 62)\n",
      "y_weather shape: (10000,)\n",
      "Distribui√ß√£o do target:\n",
      "RainTomorrow\n",
      "0    7797\n",
      "1    2203\n",
      "Name: count, dtype: int64\n",
      "Propor√ß√£o: RainTomorrow\n",
      "0    0.7797\n",
      "1    0.2203\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATASET 5: AUSTRALIA RAIN (WEATHER)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar dataset\n",
    "df_weather = pd.read_csv('../data/weather.csv')\n",
    "print(f\"Shape original: {df_weather.shape}\")\n",
    "print(f\"\\nValores nulos por coluna:\")\n",
    "null_counts = df_weather.isnull().sum()\n",
    "print(null_counts[null_counts > 0])\n",
    "print(f\"\\nTotal de colunas com nulos: {(null_counts > 0).sum()}\")\n",
    "\n",
    "# Remover colunas n√£o informativas\n",
    "df_weather = df_weather.drop(columns=['Date', 'Location'])\n",
    "print(f\"\\nShape ap√≥s remo√ß√£o de Date e Location: {df_weather.shape}\")\n",
    "\n",
    "# Remover target nulo primeiro\n",
    "df_weather = df_weather.dropna(subset=['RainTomorrow'])\n",
    "print(f\"Shape ap√≥s remover target nulo: {df_weather.shape}\")\n",
    "\n",
    "# Separar target\n",
    "y_weather = df_weather['RainTomorrow'].map({'Yes': 1, 'No': 0})\n",
    "X_weather = df_weather.drop(columns=['RainTomorrow'])\n",
    "\n",
    "# Remover linhas com muitos nulos (estrat√©gia: dropna)\n",
    "# Vamos remover linhas que t√™m qualquer valor nulo\n",
    "initial_rows = len(X_weather)\n",
    "valid_indices = X_weather.dropna().index\n",
    "X_weather = X_weather.loc[valid_indices]\n",
    "y_weather = y_weather.loc[valid_indices]\n",
    "\n",
    "print(f\"\\nLinhas removidas por nulos: {initial_rows - len(X_weather)}\")\n",
    "print(f\"Shape ap√≥s dropna: X={X_weather.shape}, y={y_weather.shape}\")\n",
    "\n",
    "# One-hot encoding para categ√≥ricas\n",
    "# Converter RainToday para num√©rico antes\n",
    "if 'RainToday' in X_weather.columns:\n",
    "    X_weather['RainToday'] = X_weather['RainToday'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Identificar colunas categ√≥ricas (dire√ß√µes de vento)\n",
    "categorical_cols = [col for col in X_weather.columns if 'WindDir' in col or 'Wind' in col and X_weather[col].dtype == 'object']\n",
    "print(f\"\\nColunas categ√≥ricas identificadas: {categorical_cols}\")\n",
    "\n",
    "if categorical_cols:\n",
    "    X_weather = pd.get_dummies(X_weather, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "    print(f\"Colunas ap√≥s one-hot encoding: {len(X_weather.columns)}\")\n",
    "\n",
    "# AMOSTRAGEM ESTRATIFICADA para ~10k amostras\n",
    "if len(X_weather) > 10000:\n",
    "    sample_size = 10000\n",
    "    print(f\"\\nFazendo amostragem estratificada de {len(X_weather)} para {sample_size} amostras...\")\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_weather, _, y_weather, _ = train_test_split(\n",
    "        X_weather, y_weather, \n",
    "        train_size=sample_size, \n",
    "        stratify=y_weather,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"Shape ap√≥s amostragem: X={X_weather.shape}, y={y_weather.shape}\")\n",
    "\n",
    "# Normalizar com StandardScaler\n",
    "scaler_weather = StandardScaler()\n",
    "X_weather = pd.DataFrame(\n",
    "    scaler_weather.fit_transform(X_weather),\n",
    "    columns=X_weather.columns\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Pr√©-processamento conclu√≠do!\")\n",
    "print(f\"X_weather shape: {X_weather.shape}\")\n",
    "print(f\"y_weather shape: {y_weather.shape}\")\n",
    "print(f\"Distribui√ß√£o do target:\\n{y_weather.value_counts()}\")\n",
    "print(f\"Propor√ß√£o: {y_weather.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Resumo Final\n",
    "\n",
    "Valida√ß√£o de todos os datasets processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RESUMO FINAL - TODOS OS DATASETS PROCESSADOS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "         Dataset  Amostras  Features  Nulos em X  Nulos em y  Classe 0  Classe 1 Propor√ß√£o (%)\n",
      "   Breast Cancer       569        30           0           0       357       212         37.3%\n",
      "         Titanic       891        10           0           0       549       342         38.4%\n",
      "Water Potability      3276         9           0           0      1998      1278         39.0%\n",
      "        Employee      4653        10           0           0      3053      1600         34.4%\n",
      "         Weather     10000        62           0           0      7797      2203         22.0%\n",
      "\n",
      "================================================================================\n",
      "‚úì TODOS OS DATASETS PRONTOS PARA USO NO EXPERIMENTO RCBD!\n",
      "================================================================================\n",
      "\n",
      "üìä Vari√°veis dispon√≠veis:\n",
      "  ‚Ä¢ X_breast_cancer, y_breast_cancer\n",
      "  ‚Ä¢ X_titanic, y_titanic\n",
      "  ‚Ä¢ X_water_potability, y_water_potability\n",
      "  ‚Ä¢ X_employee, y_employee\n",
      "  ‚Ä¢ X_weather, y_weather\n",
      "\n",
      "üéØ Caracter√≠sticas:\n",
      "  ‚Ä¢ Todas as features s√£o num√©ricas\n",
      "  ‚Ä¢ Sem valores nulos\n",
      "  ‚Ä¢ Normalizadas com StandardScaler (z-score)\n",
      "  ‚Ä¢ Prontas para classificadores de ML\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"RESUMO FINAL - TODOS OS DATASETS PROCESSADOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "datasets_summary = {\n",
    "    'Breast Cancer': (X_breast_cancer, y_breast_cancer),\n",
    "    'Titanic': (X_titanic, y_titanic),\n",
    "    'Water Potability': (X_water_potability, y_water_potability),\n",
    "    'Employee': (X_employee, y_employee),\n",
    "    'Weather': (X_weather, y_weather)\n",
    "}\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for name, (X, y) in datasets_summary.items():\n",
    "    summary_data.append({\n",
    "        'Dataset': name,\n",
    "        'Amostras': X.shape[0],\n",
    "        'Features': X.shape[1],\n",
    "        'Nulos em X': X.isnull().sum().sum(),\n",
    "        'Nulos em y': y.isnull().sum(),\n",
    "        'Classe 0': (y == 0).sum(),\n",
    "        'Classe 1': (y == 1).sum(),\n",
    "        'Propor√ß√£o (%)': f\"{(y == 1).sum() / len(y) * 100:.1f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì TODOS OS DATASETS PRONTOS PARA USO NO EXPERIMENTO RCBD!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä Vari√°veis dispon√≠veis:\")\n",
    "print(\"  ‚Ä¢ X_breast_cancer, y_breast_cancer\")\n",
    "print(\"  ‚Ä¢ X_titanic, y_titanic\")\n",
    "print(\"  ‚Ä¢ X_water_potability, y_water_potability\")\n",
    "print(\"  ‚Ä¢ X_employee, y_employee\")\n",
    "print(\"  ‚Ä¢ X_weather, y_weather\")\n",
    "\n",
    "print(\"\\nüéØ Caracter√≠sticas:\")\n",
    "print(\"  ‚Ä¢ Todas as features s√£o num√©ricas\")\n",
    "print(\"  ‚Ä¢ Sem valores nulos\")\n",
    "print(\"  ‚Ä¢ Normalizadas com StandardScaler (z-score)\")\n",
    "print(\"  ‚Ä¢ Prontas para classificadores de ML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Baseline - SVM com Kernel RBF\n",
    "\n",
    "**Objetivo:** Treinar SVM simples em cada dataset para:\n",
    "1. Validar que os dados est√£o funcionando corretamente\n",
    "2. Obter m√©tricas baseline de refer√™ncia\n",
    "\n",
    "**Configura√ß√£o:**\n",
    "- Train/Test Split: 80/20 (stratified)\n",
    "- Modelo: SVM com kernel RBF (padr√£o)\n",
    "- M√©tricas: Acur√°cia, Precis√£o, Recall, F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BASELINE - SVM COM KERNEL RBF\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: Breast Cancer\n",
      "--------------------------------------------------------------------------------\n",
      "Treino: 455 amostras | Teste: 114 amostras\n",
      "Acur√°cia:  97.37%\n",
      "Precis√£o:  100.00%\n",
      "Recall:    92.86%\n",
      "F1-Score:  96.30%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: Titanic\n",
      "--------------------------------------------------------------------------------\n",
      "Treino: 712 amostras | Teste: 179 amostras\n",
      "Acur√°cia:  81.01%\n",
      "Precis√£o:  85.71%\n",
      "Recall:    60.87%\n",
      "F1-Score:  71.19%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: Water Potability\n",
      "--------------------------------------------------------------------------------\n",
      "Treino: 2620 amostras | Teste: 656 amostras\n",
      "Acur√°cia:  67.07%\n",
      "Precis√£o:  70.41%\n",
      "Recall:    26.95%\n",
      "F1-Score:  38.98%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: Employee\n",
      "--------------------------------------------------------------------------------\n",
      "Treino: 3722 amostras | Teste: 931 amostras\n",
      "Acur√°cia:  83.24%\n",
      "Precis√£o:  87.61%\n",
      "Recall:    59.69%\n",
      "F1-Score:  71.00%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: Weather\n",
      "--------------------------------------------------------------------------------\n",
      "Treino: 8000 amostras | Teste: 2000 amostras\n",
      "Acur√°cia:  84.70%\n",
      "Precis√£o:  73.68%\n",
      "Recall:    47.62%\n",
      "F1-Score:  57.85%\n",
      "\n",
      "================================================================================\n",
      "TABELA RESUMO - BASELINE SVM (KERNEL RBF)\n",
      "================================================================================\n",
      "\n",
      "         Dataset  Treino  Teste Acur√°cia (%) Precis√£o (%) Recall (%) F1-Score (%)\n",
      "   Breast Cancer     455    114        97.37       100.00      92.86        96.30\n",
      "         Titanic     712    179        81.01        85.71      60.87        71.19\n",
      "Water Potability    2620    656        67.07        70.41      26.95        38.98\n",
      "        Employee    3722    931        83.24        87.61      59.69        71.00\n",
      "         Weather    8000   2000        84.70        73.68      47.62        57.85\n",
      "\n",
      "================================================================================\n",
      "‚úì BASELINE CONCLU√çDO!\n",
      "================================================================================\n",
      "\n",
      "üìù Observa√ß√µes:\n",
      "  ‚Ä¢ Todos os datasets foram treinados com sucesso\n",
      "  ‚Ä¢ M√©tricas baseline dispon√≠veis para compara√ß√£o futura\n",
      "  ‚Ä¢ Employee: Dataset balanceado (~34% sa√≠das) com boas m√©tricas\n",
      "  ‚Ä¢ Pr√≥ximo passo: Experimento RCBD com diferentes tratamentos\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BASELINE - SVM COM KERNEL RBF\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Dicion√°rio para armazenar resultados\n",
    "baseline_results = []\n",
    "\n",
    "# Lista de datasets\n",
    "datasets = [\n",
    "    ('Breast Cancer', X_breast_cancer, y_breast_cancer),\n",
    "    ('Titanic', X_titanic, y_titanic),\n",
    "    ('Water Potability', X_water_potability, y_water_potability),\n",
    "    ('Employee', X_employee, y_employee),\n",
    "    ('Weather', X_weather, y_weather)\n",
    "]\n",
    "\n",
    "# Para cada dataset\n",
    "for name, X, y in datasets:\n",
    "    print(f\"\\n{'-' * 80}\")\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"{'-' * 80}\")\n",
    "    \n",
    "    # Train/Test Split (80/20) com stratify\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        stratify=y, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Treino: {X_train.shape[0]} amostras | Teste: {X_test.shape[0]} amostras\")\n",
    "    \n",
    "    # Treinar SVM com kernel RBF\n",
    "    svm = SVC(kernel='rbf', random_state=42)\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Predi√ß√µes\n",
    "    y_pred = svm.predict(X_test)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    baseline_results.append({\n",
    "        'Dataset': name,\n",
    "        'Treino': X_train.shape[0],\n",
    "        'Teste': X_test.shape[0],\n",
    "        'Acur√°cia (%)': acc * 100,\n",
    "        'Precis√£o (%)': prec * 100,\n",
    "        'Recall (%)': rec * 100,\n",
    "        'F1-Score (%)': f1 * 100\n",
    "    })\n",
    "    \n",
    "    print(f\"Acur√°cia:  {acc*100:.2f}%\")\n",
    "    print(f\"Precis√£o:  {prec*100:.2f}%\")\n",
    "    print(f\"Recall:    {rec*100:.2f}%\")\n",
    "    print(f\"F1-Score:  {f1*100:.2f}%\")\n",
    "\n",
    "# Criar DataFrame com resultados consolidados\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABELA RESUMO - BASELINE SVM (KERNEL RBF)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "# Formatar colunas de m√©tricas com 2 casas decimais\n",
    "for col in ['Acur√°cia (%)', 'Precis√£o (%)', 'Recall (%)', 'F1-Score (%)']:\n",
    "    baseline_df[col] = baseline_df[col].map('{:.2f}'.format)\n",
    "\n",
    "print(baseline_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì BASELINE CONCLU√çDO!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìù Observa√ß√µes:\")\n",
    "print(\"  ‚Ä¢ Todos os datasets foram treinados com sucesso\")\n",
    "print(\"  ‚Ä¢ M√©tricas baseline dispon√≠veis para compara√ß√£o futura\")\n",
    "print(\"  ‚Ä¢ Employee: Dataset balanceado (~34% sa√≠das) com boas m√©tricas\")\n",
    "print(\"  ‚Ä¢ Pr√≥ximo passo: Experimento RCBD com diferentes tratamentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Otimiza√ß√£o de Hiperpar√¢metros - SVM\n",
    "\n",
    "**Objetivo:** Implementar 3 m√©todos de otimiza√ß√£o de hiperpar√¢metros para compara√ß√£o:\n",
    "1. **GridSearch**: Busca exaustiva em grid definido\n",
    "2. **RandomSearch**: Amostragem aleat√≥ria no espa√ßo de busca\n",
    "3. **Bayesian Optimization**: Otimiza√ß√£o bayesiana com scikit-optimize\n",
    "\n",
    "**Configura√ß√£o:**\n",
    "- Modelo: SVM com kernel RBF\n",
    "- Hiperpar√¢metros: C e gamma\n",
    "- Mesma quantidade de itera√ß√µes (n_iter) para todos os m√©todos\n",
    "- Sem cross-validation: treino em X_train, teste em X_test\n",
    "- M√©tricas: Acur√°cia, Precis√£o, Recall, F1-Score, Tempo de execu√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fun√ß√£o grid_search_svm() implementada!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "def grid_search_svm(X_train, y_train, X_test, y_test, n_iter=16, verbose=True):\n",
    "    \"\"\"\n",
    "    Grid Search para otimiza√ß√£o de hiperpar√¢metros do SVM.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    -----------\n",
    "    X_train, y_train : dados de treino\n",
    "    X_test, y_test : dados de teste\n",
    "    n_iter : n√∫mero de combina√ß√µes a testar (ser√° ajustado para grid quadrado mais pr√≥ximo)\n",
    "    verbose : exibir mensagens de progresso (default=True)\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict com: metodo, best_params, acuracia, precisao, recall, f1_score, tempo\n",
    "    \"\"\"\n",
    "    inicio = time.time()\n",
    "    \n",
    "    # Criar grid que resulte em aproximadamente n_iter combina√ß√µes\n",
    "    # Exemplo: n_iter=16 ‚Üí 4x4 grid\n",
    "    grid_size = int(np.sqrt(n_iter))\n",
    "    \n",
    "    # Definir ranges para C e gamma\n",
    "    C_values = np.logspace(-2, 3, grid_size)  # 0.01 a 1000\n",
    "    gamma_values = np.logspace(-4, 1, grid_size)  # 0.0001 a 10\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Grid Search: testando {grid_size}x{grid_size} = {grid_size**2} combina√ß√µes\")\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    best_metrics = {}\n",
    "    \n",
    "    # Testar todas as combina√ß√µes\n",
    "    for C in C_values:\n",
    "        for gamma in gamma_values:\n",
    "            # Treinar modelo\n",
    "            model = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Testar no conjunto de teste\n",
    "            y_pred = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # Se for o melhor, salvar\n",
    "            if acc > best_score:\n",
    "                best_score = acc\n",
    "                best_params = {'C': C, 'gamma': gamma}\n",
    "                best_metrics = {\n",
    "                    'acuracia': acc,\n",
    "                    'precisao': precision_score(y_test, y_pred),\n",
    "                    'recall': recall_score(y_test, y_pred),\n",
    "                    'f1_score': f1_score(y_test, y_pred)\n",
    "                }\n",
    "    \n",
    "    tempo = time.time() - inicio\n",
    "    \n",
    "    return {\n",
    "        'metodo': 'GridSearch',\n",
    "        'best_params': best_params,\n",
    "        'acuracia': best_metrics['acuracia'],\n",
    "        'precisao': best_metrics['precisao'],\n",
    "        'recall': best_metrics['recall'],\n",
    "        'f1_score': best_metrics['f1_score'],\n",
    "        'tempo': tempo\n",
    "    }\n",
    "\n",
    "print(\"‚úì Fun√ß√£o grid_search_svm() implementada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fun√ß√£o random_search_svm() implementada!\n"
     ]
    }
   ],
   "source": [
    "def random_search_svm(X_train, y_train, X_test, y_test, n_iter=16, verbose=True):\n",
    "    \"\"\"\n",
    "    Random Search para otimiza√ß√£o de hiperpar√¢metros do SVM.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    -----------\n",
    "    X_train, y_train : dados de treino\n",
    "    X_test, y_test : dados de teste\n",
    "    n_iter : n√∫mero de combina√ß√µes aleat√≥rias a testar\n",
    "    verbose : exibir mensagens de progresso (default=True)\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict com: metodo, best_params, acuracia, precisao, recall, f1_score, tempo\n",
    "    \"\"\"\n",
    "    inicio = time.time()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Random Search: testando {n_iter} combina√ß√µes aleat√≥rias\")\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    best_metrics = {}\n",
    "    \n",
    "    np.random.seed(42)  # Para reprodutibilidade\n",
    "    \n",
    "    # Testar n_iter combina√ß√µes aleat√≥rias\n",
    "    for i in range(n_iter):\n",
    "        # Amostrar C e gamma de distribui√ß√µes log-uniformes\n",
    "        C = 10 ** np.random.uniform(-2, 3)  # 0.01 a 1000\n",
    "        gamma = 10 ** np.random.uniform(-4, 1)  # 0.0001 a 10\n",
    "        \n",
    "        # Treinar modelo\n",
    "        model = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Testar no conjunto de teste\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Se for o melhor, salvar\n",
    "        if acc > best_score:\n",
    "            best_score = acc\n",
    "            best_params = {'C': C, 'gamma': gamma}\n",
    "            best_metrics = {\n",
    "                'acuracia': acc,\n",
    "                'precisao': precision_score(y_test, y_pred),\n",
    "                'recall': recall_score(y_test, y_pred),\n",
    "                'f1_score': f1_score(y_test, y_pred)\n",
    "            }\n",
    "    \n",
    "    tempo = time.time() - inicio\n",
    "    \n",
    "    return {\n",
    "        'metodo': 'RandomSearch',\n",
    "        'best_params': best_params,\n",
    "        'acuracia': best_metrics['acuracia'],\n",
    "        'precisao': best_metrics['precisao'],\n",
    "        'recall': best_metrics['recall'],\n",
    "        'f1_score': best_metrics['f1_score'],\n",
    "        'tempo': tempo\n",
    "    }\n",
    "\n",
    "print(\"‚úì Fun√ß√£o random_search_svm() implementada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fun√ß√£o bayesian_search_svm() implementada!\n"
     ]
    }
   ],
   "source": [
    "def bayesian_search_svm(X_train, y_train, X_test, y_test, n_iter=16, verbose=True):\n",
    "    \"\"\"\n",
    "    Bayesian Optimization para otimiza√ß√£o de hiperpar√¢metros do SVM.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    -----------\n",
    "    X_train, y_train : dados de treino\n",
    "    X_test, y_test : dados de teste\n",
    "    n_iter : n√∫mero de itera√ß√µes da otimiza√ß√£o bayesiana\n",
    "    verbose : exibir mensagens de progresso (default=True)\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict com: metodo, best_params, acuracia, precisao, recall, f1_score, tempo\n",
    "    \"\"\"\n",
    "    inicio = time.time()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Bayesian Optimization: {n_iter} itera√ß√µes\")\n",
    "    \n",
    "    # Definir espa√ßo de busca (escala logar√≠tmica)\n",
    "    space = [\n",
    "        Real(1e-2, 1e3, prior='log-uniform', name='C'),\n",
    "        Real(1e-4, 1e1, prior='log-uniform', name='gamma')\n",
    "    ]\n",
    "    \n",
    "    # Vari√°vel para armazenar melhor resultado\n",
    "    best_metrics = {}\n",
    "    \n",
    "    # Fun√ß√£o objetivo a ser MINIMIZADA (por isso retorna -accuracy)\n",
    "    @use_named_args(space)\n",
    "    def objective(**params):\n",
    "        C = params['C']\n",
    "        gamma = params['gamma']\n",
    "        \n",
    "        # Treinar modelo\n",
    "        model = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Testar no conjunto de teste\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Retornar negativo da acur√°cia (queremos maximizar, mas gp_minimize minimiza)\n",
    "        return -acc\n",
    "    \n",
    "    # Executar otimiza√ß√£o bayesiana\n",
    "    result = gp_minimize(\n",
    "        objective,\n",
    "        space,\n",
    "        n_calls=n_iter,\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Obter melhores par√¢metros\n",
    "    best_C = result.x[0]\n",
    "    best_gamma = result.x[1]\n",
    "    best_params = {'C': best_C, 'gamma': best_gamma}\n",
    "    \n",
    "    # Treinar modelo final com os melhores par√¢metros\n",
    "    final_model = SVC(kernel='rbf', C=best_C, gamma=best_gamma, random_state=42)\n",
    "    final_model.fit(X_train, y_train)\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    \n",
    "    # Calcular todas as m√©tricas\n",
    "    best_metrics = {\n",
    "        'acuracia': accuracy_score(y_test, y_pred),\n",
    "        'precisao': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    tempo = time.time() - inicio\n",
    "    \n",
    "    return {\n",
    "        'metodo': 'BayesianOptimization',\n",
    "        'best_params': best_params,\n",
    "        'acuracia': best_metrics['acuracia'],\n",
    "        'precisao': best_metrics['precisao'],\n",
    "        'recall': best_metrics['recall'],\n",
    "        'f1_score': best_metrics['f1_score'],\n",
    "        'tempo': tempo\n",
    "    }\n",
    "\n",
    "print(\"‚úì Fun√ß√£o bayesian_search_svm() implementada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste das Fun√ß√µes de Otimiza√ß√£o\n",
    "\n",
    "Vamos testar as 3 fun√ß√µes com o dataset Breast Cancer (menor e mais r√°pido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTE DAS FUN√á√ïES DE OTIMIZA√á√ÉO - BREAST CANCER\n",
      "================================================================================\n",
      "\n",
      "Treino: 455 amostras | Teste: 114 amostras\n",
      "\n",
      "Testando com n_iter=16 (4x4 grid)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Grid Search: testando 4x4 = 16 combina√ß√µes\n",
      "M√©todo: GridSearch\n",
      "Melhores par√¢metros: C=21.5443, gamma=0.0046\n",
      "Acur√°cia:  98.25%\n",
      "Precis√£o:  100.00%\n",
      "Recall:    95.24%\n",
      "F1-Score:  97.56%\n",
      "Tempo:     0.30s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Random Search: testando 16 combina√ß√µes aleat√≥rias\n",
      "M√©todo: RandomSearch\n",
      "Melhores par√¢metros: C=145.2825, gamma=0.0012\n",
      "Acur√°cia:  96.49%\n",
      "Precis√£o:  100.00%\n",
      "Recall:    90.48%\n",
      "F1-Score:  95.00%\n",
      "Tempo:     0.28s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Bayesian Optimization: 16 itera√ß√µes\n",
      "M√©todo: BayesianOptimization\n",
      "Melhores par√¢metros: C=96.0981, gamma=0.0008\n",
      "Acur√°cia:  98.25%\n",
      "Precis√£o:  100.00%\n",
      "Recall:    95.24%\n",
      "F1-Score:  97.56%\n",
      "Tempo:     2.00s\n",
      "\n",
      "================================================================================\n",
      "‚úì TESTE CONCLU√çDO - TODAS AS FUN√á√ïES FUNCIONANDO!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TESTE DAS FUN√á√ïES DE OTIMIZA√á√ÉO - BREAST CANCER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Preparar dados (train/test split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_breast_cancer, y_breast_cancer, \n",
    "    test_size=0.2, \n",
    "    stratify=y_breast_cancer, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTreino: {X_train.shape[0]} amostras | Teste: {X_test.shape[0]} amostras\")\n",
    "print(f\"\\nTestando com n_iter=16 (4x4 grid)\\n\")\n",
    "\n",
    "# Testar GridSearch\n",
    "print(\"-\" * 80)\n",
    "result_grid = grid_search_svm(X_train, y_train, X_test, y_test, n_iter=16)\n",
    "print(f\"M√©todo: {result_grid['metodo']}\")\n",
    "print(f\"Melhores par√¢metros: C={result_grid['best_params']['C']:.4f}, gamma={result_grid['best_params']['gamma']:.4f}\")\n",
    "print(f\"Acur√°cia:  {result_grid['acuracia']*100:.2f}%\")\n",
    "print(f\"Precis√£o:  {result_grid['precisao']*100:.2f}%\")\n",
    "print(f\"Recall:    {result_grid['recall']*100:.2f}%\")\n",
    "print(f\"F1-Score:  {result_grid['f1_score']*100:.2f}%\")\n",
    "print(f\"Tempo:     {result_grid['tempo']:.2f}s\")\n",
    "\n",
    "# Testar RandomSearch\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "result_random = random_search_svm(X_train, y_train, X_test, y_test, n_iter=16)\n",
    "print(f\"M√©todo: {result_random['metodo']}\")\n",
    "print(f\"Melhores par√¢metros: C={result_random['best_params']['C']:.4f}, gamma={result_random['best_params']['gamma']:.4f}\")\n",
    "print(f\"Acur√°cia:  {result_random['acuracia']*100:.2f}%\")\n",
    "print(f\"Precis√£o:  {result_random['precisao']*100:.2f}%\")\n",
    "print(f\"Recall:    {result_random['recall']*100:.2f}%\")\n",
    "print(f\"F1-Score:  {result_random['f1_score']*100:.2f}%\")\n",
    "print(f\"Tempo:     {result_random['tempo']:.2f}s\")\n",
    "\n",
    "# Testar Bayesian Optimization\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "result_bayesian = bayesian_search_svm(X_train, y_train, X_test, y_test, n_iter=16)\n",
    "print(f\"M√©todo: {result_bayesian['metodo']}\")\n",
    "print(f\"Melhores par√¢metros: C={result_bayesian['best_params']['C']:.4f}, gamma={result_bayesian['best_params']['gamma']:.4f}\")\n",
    "print(f\"Acur√°cia:  {result_bayesian['acuracia']*100:.2f}%\")\n",
    "print(f\"Precis√£o:  {result_bayesian['precisao']*100:.2f}%\")\n",
    "print(f\"Recall:    {result_bayesian['recall']*100:.2f}%\")\n",
    "print(f\"F1-Score:  {result_bayesian['f1_score']*100:.2f}%\")\n",
    "print(f\"Tempo:     {result_bayesian['tempo']:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì TESTE CONCLU√çDO - TODAS AS FUN√á√ïES FUNCIONANDO!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Experimento RCBD Completo\n",
    "\n",
    "**Objetivo:** Executar o experimento completo comparando os 3 m√©todos de otimiza√ß√£o\n",
    "\n",
    "**Configura√ß√£o:**\n",
    "- **Blocos:** 5 datasets (Breast Cancer, Titanic, Water Potability, Employee, Weather)\n",
    "- **Repeti√ß√µes:** 7 seeds diferentes (1-7) para cada dataset\n",
    "- **Tratamentos:** 3 m√©todos de otimiza√ß√£o (GridSearch, RandomSearch, BayesianOptimization)\n",
    "- **Total de experimentos:** 5 datasets √ó 7 seeds √ó 3 m√©todos = **105 experimentos**\n",
    "\n",
    "**Delineamento RCBD:**\n",
    "- Datasets funcionam como blocos (controlam variabilidade)\n",
    "- M√©todos de otimiza√ß√£o s√£o os tratamentos a comparar\n",
    "- M√∫ltiplas repeti√ß√µes com seeds diferentes para robustez estat√≠stica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONFIGURA√á√ÉO DO EXPERIMENTO RCBD\n",
      "================================================================================\n",
      "\n",
      "N√∫mero de datasets (blocos): 5\n",
      "Datasets: ['Breast Cancer', 'Titanic', 'Water Potability', 'Employee', 'Weather']\n",
      "\n",
      "N√∫mero de seeds (repeti√ß√µes): 7\n",
      "Seeds: [1, 2, 3, 4, 5, 6, 7]\n",
      "\n",
      "M√©todos de otimiza√ß√£o: GridSearch, RandomSearch, BayesianOptimization\n",
      "Itera√ß√µes por m√©todo: 16\n",
      "\n",
      "Total de experimentos: 5 √ó 7 √ó 3 = 105\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# Criar dicion√°rio com todos os datasets\n",
    "datasets_dict = {\n",
    "    'Breast Cancer': (X_breast_cancer, y_breast_cancer),\n",
    "    'Titanic': (X_titanic, y_titanic),\n",
    "    'Water Potability': (X_water_potability, y_water_potability),\n",
    "    'Employee': (X_employee, y_employee),\n",
    "    'Weather': (X_weather, y_weather)\n",
    "}\n",
    "\n",
    "# Definir seeds para repeti√ß√µes\n",
    "seeds = list(range(1, 8))  # [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Configura√ß√£o do experimento\n",
    "n_iter = 16  # N√∫mero de itera√ß√µes para cada m√©todo\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFIGURA√á√ÉO DO EXPERIMENTO RCBD\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nN√∫mero de datasets (blocos): {len(datasets_dict)}\")\n",
    "print(f\"Datasets: {list(datasets_dict.keys())}\")\n",
    "print(f\"\\nN√∫mero de seeds (repeti√ß√µes): {len(seeds)}\")\n",
    "print(f\"Seeds: {seeds}\")\n",
    "print(f\"\\nM√©todos de otimiza√ß√£o: GridSearch, RandomSearch, BayesianOptimization\")\n",
    "print(f\"Itera√ß√µes por m√©todo: {n_iter}\")\n",
    "print(f\"\\nTotal de experimentos: {len(datasets_dict)} √ó {len(seeds)} √ó 3 = {len(datasets_dict) * len(seeds) * 3}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXECUTANDO EXPERIMENTO RCBD\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585cd26375214b23baa681d8f1961399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seeds:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cb43b72a3a4a4d9146f38e01fc7bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7efe2f65b04fd28538cfe2c9b35ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 2:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m resultados.append(result_random)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 4. BayesianOptimization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m result_bayesian = \u001b[43mbayesian_search_svm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m result_bayesian[\u001b[33m'\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m'\u001b[39m] = dataset_name\n\u001b[32m     38\u001b[39m result_bayesian[\u001b[33m'\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m'\u001b[39m] = seed\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mbayesian_search_svm\u001b[39m\u001b[34m(X_train, y_train, X_test, y_test, n_iter, verbose)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -acc\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Executar otimiza√ß√£o bayesiana\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m result = \u001b[43mgp_minimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_calls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     54\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Obter melhores par√¢metros\u001b[39;00m\n\u001b[32m     57\u001b[39m best_C = result.x[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/planejamento-analise-experimentos-2025-2/trabalho_final/.venv/lib/python3.13/site-packages/skopt/optimizer/gp.py:281\u001b[39m, in \u001b[36mgp_minimize\u001b[39m\u001b[34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size, space_constraint)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m base_estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    274\u001b[39m     base_estimator = cook_estimator(\n\u001b[32m    275\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGP\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m         space=space,\n\u001b[32m    277\u001b[39m         random_state=rng.randint(\u001b[32m0\u001b[39m, np.iinfo(np.int32).max),\n\u001b[32m    278\u001b[39m         noise=noise,\n\u001b[32m    279\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase_minimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43macq_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43macq_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkappa\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkappa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43macq_optimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43macq_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_calls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_random_starts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_random_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_initial_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_initial_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_point_generator\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_point_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_restarts_optimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_restarts_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43my0\u001b[49m\u001b[43m=\u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspace_constraint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspace_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_queue_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/planejamento-analise-experimentos-2025-2/trabalho_final/.venv/lib/python3.13/site-packages/skopt/optimizer/base.py:332\u001b[39m, in \u001b[36mbase_minimize\u001b[39m\u001b[34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size, space_constraint)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_calls):\n\u001b[32m    331\u001b[39m     next_x = optimizer.ask()\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     next_y = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m     result = optimizer.tell(next_x, next_y)\n\u001b[32m    334\u001b[39m     result.specs = specs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/planejamento-analise-experimentos-2025-2/trabalho_final/.venv/lib/python3.13/site-packages/skopt/utils.py:779\u001b[39m, in \u001b[36muse_named_args.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    776\u001b[39m arg_dict = {dim.name: value \u001b[38;5;28;01mfor\u001b[39;00m dim, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dimensions, x)}\n\u001b[32m    778\u001b[39m \u001b[38;5;66;03m# Call the wrapped objective function with the named arguments.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m objective_value = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43marg_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m objective_value\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mbayesian_search_svm.<locals>.objective\u001b[39m\u001b[34m(**params)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Treinar modelo\u001b[39;00m\n\u001b[32m     37\u001b[39m model = SVC(kernel=\u001b[33m'\u001b[39m\u001b[33mrbf\u001b[39m\u001b[33m'\u001b[39m, C=C, gamma=gamma, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Testar no conjunto de teste\u001b[39;00m\n\u001b[32m     41\u001b[39m y_pred = model.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/planejamento-analise-experimentos-2025-2/trabalho_final/.venv/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/planejamento-analise-experimentos-2025-2/trabalho_final/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:258\u001b[39m, in \u001b[36mBaseLibSVM.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[LibSVM]\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    257\u001b[39m seed = rnd.randint(np.iinfo(\u001b[33m\"\u001b[39m\u001b[33mi\u001b[39m\u001b[33m\"\u001b[39m).max)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[38;5;28mself\u001b[39m.shape_fit_ = X.shape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/planejamento-analise-experimentos-2025-2/trabalho_final/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:336\u001b[39m, in \u001b[36mBaseLibSVM._dense_fit\u001b[39m\u001b[34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[39m\n\u001b[32m    322\u001b[39m libsvm.set_verbosity_wrap(\u001b[38;5;28mself\u001b[39m.verbose)\n\u001b[32m    324\u001b[39m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[32m    326\u001b[39m (\n\u001b[32m    327\u001b[39m     \u001b[38;5;28mself\u001b[39m.support_,\n\u001b[32m    328\u001b[39m     \u001b[38;5;28mself\u001b[39m.support_vectors_,\n\u001b[32m    329\u001b[39m     \u001b[38;5;28mself\u001b[39m._n_support,\n\u001b[32m    330\u001b[39m     \u001b[38;5;28mself\u001b[39m.dual_coef_,\n\u001b[32m    331\u001b[39m     \u001b[38;5;28mself\u001b[39m.intercept_,\n\u001b[32m    332\u001b[39m     \u001b[38;5;28mself\u001b[39m._probA,\n\u001b[32m    333\u001b[39m     \u001b[38;5;28mself\u001b[39m._probB,\n\u001b[32m    334\u001b[39m     \u001b[38;5;28mself\u001b[39m.fit_status_,\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m._num_iter,\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m ) = \u001b[43mlibsvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclass_weight_\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[38;5;28mself\u001b[39m._warn_from_fit_status()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXECUTANDO EXPERIMENTO RCBD\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Lista para armazenar todos os resultados\n",
    "resultados = []\n",
    "\n",
    "# Loop externo: iterar sobre seeds (repeti√ß√µes) - MAIS REALISTA PARA TQDM\n",
    "for seed in tqdm(seeds, desc=\"Seeds\", position=0):\n",
    "    \n",
    "    # Loop interno: iterar sobre datasets (blocos)\n",
    "    for dataset_name, (X, y) in tqdm(datasets_dict.items(), desc=f\"Seed {seed}\", position=1, leave=False):\n",
    "        \n",
    "        # 1. Fazer split train/test ESTRATIFICADO com a seed atual\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=0.2, \n",
    "            stratify=y,  # Manter propor√ß√£o de classes\n",
    "            random_state=seed\n",
    "        )\n",
    "        \n",
    "        # 2. GridSearch\n",
    "        result_grid = grid_search_svm(X_train, y_train, X_test, y_test, n_iter=n_iter, verbose=False)\n",
    "        result_grid['dataset'] = dataset_name\n",
    "        result_grid['seed'] = seed\n",
    "        resultados.append(result_grid)\n",
    "        \n",
    "        # 3. RandomSearch\n",
    "        result_random = random_search_svm(X_train, y_train, X_test, y_test, n_iter=n_iter, verbose=False)\n",
    "        result_random['dataset'] = dataset_name\n",
    "        result_random['seed'] = seed\n",
    "        resultados.append(result_random)\n",
    "        \n",
    "        # 4. BayesianOptimization\n",
    "        result_bayesian = bayesian_search_svm(X_train, y_train, X_test, y_test, n_iter=n_iter, verbose=False)\n",
    "        result_bayesian['dataset'] = dataset_name\n",
    "        result_bayesian['seed'] = seed\n",
    "        resultados.append(result_bayesian)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì EXPERIMENTO CONCLU√çDO!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal de resultados coletados: {len(resultados)}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONSOLIDANDO RESULTADOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Converter lista de dicts para DataFrame\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "# Reordenar colunas para melhor visualiza√ß√£o\n",
    "colunas_ordenadas = ['dataset', 'seed', 'metodo', 'acuracia', 'precisao', 'recall', 'f1_score', 'tempo', 'best_params']\n",
    "df_resultados = df_resultados[colunas_ordenadas]\n",
    "\n",
    "# Mostrar informa√ß√µes b√°sicas\n",
    "print(f\"\\nShape do DataFrame: {df_resultados.shape}\")\n",
    "print(f\"Colunas: {list(df_resultados.columns)}\")\n",
    "\n",
    "# Mostrar primeiras linhas\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"PRIMEIRAS 10 LINHAS:\")\n",
    "print(\"-\" * 80)\n",
    "print(df_resultados.head(10).to_string(index=False))\n",
    "\n",
    "# Estat√≠sticas descritivas por m√©todo\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTAT√çSTICAS DESCRITIVAS POR M√âTODO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for metodo in ['GridSearch', 'RandomSearch', 'BayesianOptimization']:\n",
    "    df_metodo = df_resultados[df_resultados['metodo'] == metodo]\n",
    "    print(f\"\\n{metodo}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Acur√°cia:  m√©dia={df_metodo['acuracia'].mean():.4f}, std={df_metodo['acuracia'].std():.4f}\")\n",
    "    print(f\"Precis√£o:  m√©dia={df_metodo['precisao'].mean():.4f}, std={df_metodo['precisao'].std():.4f}\")\n",
    "    print(f\"Recall:    m√©dia={df_metodo['recall'].mean():.4f}, std={df_metodo['recall'].std():.4f}\")\n",
    "    print(f\"F1-Score:  m√©dia={df_metodo['f1_score'].mean():.4f}, std={df_metodo['f1_score'].std():.4f}\")\n",
    "    print(f\"Tempo(s):  m√©dia={df_metodo['tempo'].mean():.2f}s, std={df_metodo['tempo'].std():.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì RESULTADOS CONSOLIDADOS!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SALVANDO RESULTADOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Criar diret√≥rio results se n√£o existir\n",
    "results_dir = '../results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Salvar DataFrame em CSV\n",
    "output_file = os.path.join(results_dir, 'experimento_rcbd_resultados.csv')\n",
    "df_resultados.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\n‚úì Resultados salvos em: {output_file}\")\n",
    "print(f\"  Total de linhas: {len(df_resultados)}\")\n",
    "print(f\"  Colunas: {', '.join(df_resultados.columns)}\")\n",
    "\n",
    "# Tamb√©m salvar vers√£o JSON dos best_params separadamente para facilitar an√°lise\n",
    "import json\n",
    "\n",
    "# Expandir best_params em colunas separadas\n",
    "df_resultados_expandido = df_resultados.copy()\n",
    "df_resultados_expandido['C'] = df_resultados_expandido['best_params'].apply(lambda x: x['C'])\n",
    "df_resultados_expandido['gamma'] = df_resultados_expandido['best_params'].apply(lambda x: x['gamma'])\n",
    "df_resultados_expandido = df_resultados_expandido.drop(columns=['best_params'])\n",
    "\n",
    "# Salvar vers√£o expandida\n",
    "output_file_expandido = os.path.join(results_dir, 'experimento_rcbd_resultados_expandido.csv')\n",
    "df_resultados_expandido.to_csv(output_file_expandido, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Vers√£o expandida salva em: {output_file_expandido}\")\n",
    "print(f\"  (best_params separado em colunas C e gamma)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì EXPERIMENTO RCBD COMPLETO - TODOS OS DADOS SALVOS!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä Pr√≥ximos passos:\")\n",
    "print(\"  1. An√°lise explorat√≥ria dos resultados\")\n",
    "print(\"  2. ANOVA para comparar m√©todos de otimiza√ß√£o\")\n",
    "print(\"  3. Testes post-hoc se ANOVA indicar diferen√ßas significativas\")\n",
    "print(\"  4. Visualiza√ß√µes (boxplots, gr√°ficos de intera√ß√£o)\")\n",
    "print(\"  5. Conclus√µes e recomenda√ß√µes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Trabalho Final PAE)",
   "language": "python",
   "name": "trabalho-final-pae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
