{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√©-processamento dos Datasets para Experimento RCBD\n",
    "\n",
    "**Equipe F**: Bernardo Bacha de Resende, Gustavo Augusto Faria dos Reis, Mar√≠lia Mac√™do de Melo\n",
    "\n",
    "**Disciplina**: EEE933 - Planejamento e An√°lise de Experimentos (2025/2)\n",
    "\n",
    "---\n",
    "\n",
    "Este notebook processa os 5 datasets de classifica√ß√£o bin√°ria, preparando-os para uso no experimento RCBD.\n",
    "\n",
    "**Datasets:**\n",
    "1. Breast Cancer (569 amostras)\n",
    "2. Titanic (891 amostras)\n",
    "3. Water Potability (3,276 amostras)\n",
    "4. Employee Attrition (4,653 amostras)\n",
    "5. Australia Rain (145,460 amostras ‚Üí amostragem para ~10k)\n",
    "\n",
    "**Pr√©-processamento aplicado:**\n",
    "- Remo√ß√£o de colunas n√£o informativas (IDs, nomes, etc.)\n",
    "- Tratamento de valores nulos (mediana para num√©rico, moda para categ√≥rico)\n",
    "- One-hot encoding para features categ√≥ricas\n",
    "- StandardScaler (z-score) para normaliza√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports e Configura√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Bibliotecas importadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì scikit-optimize j√° instalado\n"
     ]
    }
   ],
   "source": [
    "# Instalar scikit-optimize se necess√°rio\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import skopt\n",
    "    print(\"‚úì scikit-optimize j√° instalado\")\n",
    "except ImportError:\n",
    "    print(\"Instalando scikit-optimize...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-optimize\"])\n",
    "    print(\"‚úì scikit-optimize instalado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset 1: Breast Cancer\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- 569 amostras √ó 32 colunas\n",
    "- Target: `diagnosis` (M=Malignant, B=Benign)\n",
    "- Sem valores nulos\n",
    "- Dataset mais limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET 1: BREAST CANCER\n",
      "================================================================================\n",
      "Shape original: (569, 32)\n",
      "Colunas: ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n",
      "\n",
      "Valores nulos: 0\n",
      "\n",
      "‚úì Pr√©-processamento conclu√≠do!\n",
      "X_breast_cancer shape: (569, 30)\n",
      "y_breast_cancer shape: (569,)\n",
      "Distribui√ß√£o do target:\n",
      "diagnosis\n",
      "0    357\n",
      "1    212\n",
      "Name: count, dtype: int64\n",
      "Propor√ß√£o: diagnosis\n",
      "0    0.627417\n",
      "1    0.372583\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATASET 1: BREAST CANCER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar dataset\n",
    "df_breast = pd.read_csv('../data/breast_cancer.csv')\n",
    "print(f\"Shape original: {df_breast.shape}\")\n",
    "print(f\"Colunas: {list(df_breast.columns)}\")\n",
    "\n",
    "# Remover coluna ID\n",
    "df_breast = df_breast.drop(columns=['id'])\n",
    "\n",
    "# Separar target\n",
    "y_breast_cancer = df_breast['diagnosis'].map({'M': 1, 'B': 0})\n",
    "X_breast_cancer = df_breast.drop(columns=['diagnosis'])\n",
    "\n",
    "# Verificar nulos\n",
    "print(f\"\\nValores nulos: {X_breast_cancer.isnull().sum().sum()}\")\n",
    "\n",
    "# Normalizar com StandardScaler\n",
    "scaler_breast = StandardScaler()\n",
    "X_breast_cancer = pd.DataFrame(\n",
    "    scaler_breast.fit_transform(X_breast_cancer),\n",
    "    columns=X_breast_cancer.columns\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Pr√©-processamento conclu√≠do!\")\n",
    "print(f\"X_breast_cancer shape: {X_breast_cancer.shape}\")\n",
    "print(f\"y_breast_cancer shape: {y_breast_cancer.shape}\")\n",
    "print(f\"Distribui√ß√£o do target:\\n{y_breast_cancer.value_counts()}\")\n",
    "print(f\"Propor√ß√£o: {y_breast_cancer.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset 2: Titanic\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- 891 amostras √ó 12 colunas\n",
    "- Target: `Survived` (0/1)\n",
    "- Valores nulos em Age, Cabin, Embarked\n",
    "- Features categ√≥ricas: Sex, Embarked, Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET 2: TITANIC\n",
      "================================================================================\n",
      "Shape original: (891, 12)\n",
      "\n",
      "Valores nulos por coluna:\n",
      "Age         177\n",
      "Cabin       687\n",
      "Embarked      2\n",
      "dtype: int64\n",
      "\n",
      "Valores nulos ap√≥s tratamento: 0\n",
      "\n",
      "Colunas ap√≥s one-hot encoding (10): ['Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Embarked_Q', 'Embarked_S', 'Pclass_1', 'Pclass_2', 'Pclass_3']\n",
      "\n",
      "‚úì Pr√©-processamento conclu√≠do!\n",
      "X_titanic shape: (891, 10)\n",
      "y_titanic shape: (891,)\n",
      "Distribui√ß√£o do target:\n",
      "Survived\n",
      "0    549\n",
      "1    342\n",
      "Name: count, dtype: int64\n",
      "Propor√ß√£o: Survived\n",
      "0    0.616162\n",
      "1    0.383838\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATASET 2: TITANIC\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar dataset\n",
    "df_titanic = pd.read_csv('../data/titanic.csv')\n",
    "print(f\"Shape original: {df_titanic.shape}\")\n",
    "print(f\"\\nValores nulos por coluna:\\n{df_titanic.isnull().sum()[df_titanic.isnull().sum() > 0]}\")\n",
    "\n",
    "# Remover colunas n√£o informativas\n",
    "df_titanic = df_titanic.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\n",
    "\n",
    "# Separar target\n",
    "y_titanic = df_titanic['Survived']\n",
    "X_titanic = df_titanic.drop(columns=['Survived'])\n",
    "\n",
    "# Tratar valores nulos\n",
    "# Age: preencher com mediana\n",
    "X_titanic['Age'].fillna(X_titanic['Age'].median(), inplace=True)\n",
    "# Embarked: preencher com moda\n",
    "X_titanic['Embarked'].fillna(X_titanic['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "print(f\"\\nValores nulos ap√≥s tratamento: {X_titanic.isnull().sum().sum()}\")\n",
    "\n",
    "# One-hot encoding para categ√≥ricas\n",
    "categorical_cols = ['Sex', 'Embarked']\n",
    "X_titanic = pd.get_dummies(X_titanic, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "\n",
    "# Converter Pclass para dummy se ainda n√£o for num√©rico adequado\n",
    "# Pclass j√° √© num√©rico (1, 2, 3), mas pode fazer one-hot se preferir\n",
    "X_titanic = pd.get_dummies(X_titanic, columns=['Pclass'], prefix='Pclass', dtype=int)\n",
    "\n",
    "print(f\"\\nColunas ap√≥s one-hot encoding ({len(X_titanic.columns)}): {list(X_titanic.columns)}\")\n",
    "\n",
    "# Normalizar com StandardScaler\n",
    "scaler_titanic = StandardScaler()\n",
    "X_titanic = pd.DataFrame(\n",
    "    scaler_titanic.fit_transform(X_titanic),\n",
    "    columns=X_titanic.columns\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Pr√©-processamento conclu√≠do!\")\n",
    "print(f\"X_titanic shape: {X_titanic.shape}\")\n",
    "print(f\"y_titanic shape: {y_titanic.shape}\")\n",
    "print(f\"Distribui√ß√£o do target:\\n{y_titanic.value_counts()}\")\n",
    "print(f\"Propor√ß√£o: {y_titanic.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset 3: Water Potability\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- 3,276 amostras √ó 10 colunas\n",
    "- Target: `Potability` (0/1)\n",
    "- Valores nulos em pH, Sulfate, Trihalomethanes\n",
    "- Todas features num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET 3: WATER POTABILITY\n",
      "================================================================================\n",
      "Shape original: (3276, 10)\n",
      "\n",
      "Valores nulos por coluna:\n",
      "ph                 491\n",
      "Sulfate            781\n",
      "Trihalomethanes    162\n",
      "dtype: int64\n",
      "Preenchidos ph com mediana: 7.04\n",
      "Preenchidos Sulfate com mediana: 333.07\n",
      "Preenchidos Trihalomethanes com mediana: 66.62\n",
      "\n",
      "Valores nulos ap√≥s tratamento: 0\n",
      "\n",
      "‚úì Pr√©-processamento conclu√≠do!\n",
      "X_water_potability shape: (3276, 9)\n",
      "y_water_potability shape: (3276,)\n",
      "Distribui√ß√£o do target:\n",
      "Potability\n",
      "0    1998\n",
      "1    1278\n",
      "Name: count, dtype: int64\n",
      "Propor√ß√£o: Potability\n",
      "0    0.60989\n",
      "1    0.39011\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATASET 3: WATER POTABILITY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar dataset\n",
    "df_water = pd.read_csv('../data/water_potability.csv')\n",
    "print(f\"Shape original: {df_water.shape}\")\n",
    "print(f\"\\nValores nulos por coluna:\\n{df_water.isnull().sum()[df_water.isnull().sum() > 0]}\")\n",
    "\n",
    "# Separar target\n",
    "y_water_potability = df_water['Potability']\n",
    "X_water_potability = df_water.drop(columns=['Potability'])\n",
    "\n",
    "# Tratar valores nulos (preencher com mediana)\n",
    "for col in X_water_potability.columns:\n",
    "    if X_water_potability[col].isnull().sum() > 0:\n",
    "        X_water_potability[col].fillna(X_water_potability[col].median(), inplace=True)\n",
    "        print(f\"Preenchidos {col} com mediana: {X_water_potability[col].median():.2f}\")\n",
    "\n",
    "print(f\"\\nValores nulos ap√≥s tratamento: {X_water_potability.isnull().sum().sum()}\")\n",
    "\n",
    "# Normalizar com StandardScaler\n",
    "scaler_water = StandardScaler()\n",
    "X_water_potability = pd.DataFrame(\n",
    "    scaler_water.fit_transform(X_water_potability),\n",
    "    columns=X_water_potability.columns\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Pr√©-processamento conclu√≠do!\")\n",
    "print(f\"X_water_potability shape: {X_water_potability.shape}\")\n",
    "print(f\"y_water_potability shape: {y_water_potability.shape}\")\n",
    "print(f\"Distribui√ß√£o do target:\\n{y_water_potability.value_counts()}\")\n",
    "print(f\"Propor√ß√£o: {y_water_potability.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset 4: Employee Attrition\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- 4,653 amostras √ó 9 colunas\n",
    "- Target: `LeaveOrNot` (0=Ficou, 1=Saiu do emprego)\n",
    "- Sem valores nulos\n",
    "- Features categ√≥ricas: Education, City, Gender, EverBenched\n",
    "- **Aten√ß√£o**: Classes razoavelmente balanceadas (~34% sa√≠das)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET 4: EMPLOYEE ATTRITION\n",
      "================================================================================\n",
      "Shape original: (4653, 9)\n",
      "\n",
      "Valores nulos por coluna:\n",
      "0 (nenhum!)\n",
      "\n",
      "Valores nulos: 0\n",
      "\n",
      "Colunas categ√≥ricas para encoding: ['Education', 'City', 'Gender', 'EverBenched']\n",
      "\n",
      "Colunas ap√≥s one-hot encoding (10): ['JoiningYear', 'PaymentTier', 'Age', 'ExperienceInCurrentDomain', 'Education_Masters', 'Education_PHD', 'City_New Delhi', 'City_Pune', 'Gender_Male', 'EverBenched_Yes']\n",
      "\n",
      "‚úì Pr√©-processamento conclu√≠do!\n",
      "X_employee shape: (4653, 10)\n",
      "y_employee shape: (4653,)\n",
      "Distribui√ß√£o do target:\n",
      "LeaveOrNot\n",
      "0    3053\n",
      "1    1600\n",
      "Name: count, dtype: int64\n",
      "Propor√ß√£o: LeaveOrNot\n",
      "0    0.656136\n",
      "1    0.343864\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATASET 4: EMPLOYEE ATTRITION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar dataset\n",
    "df_employee = pd.read_csv('../data/Employee.csv')\n",
    "print(f\"Shape original: {df_employee.shape}\")\n",
    "print(f\"\\nValores nulos por coluna:\\n{df_employee.isnull().sum().sum()} (nenhum!)\")\n",
    "\n",
    "# Separar target\n",
    "y_employee = df_employee['LeaveOrNot']\n",
    "X_employee = df_employee.drop(columns=['LeaveOrNot'])\n",
    "\n",
    "print(f\"\\nValores nulos: {X_employee.isnull().sum().sum()}\")\n",
    "\n",
    "# Identificar e fazer one-hot encoding para categ√≥ricas\n",
    "categorical_cols = ['Education', 'City', 'Gender', 'EverBenched']\n",
    "print(f\"\\nColunas categ√≥ricas para encoding: {categorical_cols}\")\n",
    "\n",
    "X_employee = pd.get_dummies(X_employee, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "\n",
    "print(f\"\\nColunas ap√≥s one-hot encoding ({len(X_employee.columns)}): {list(X_employee.columns)}\")\n",
    "\n",
    "# Normalizar com StandardScaler\n",
    "scaler_employee = StandardScaler()\n",
    "X_employee = pd.DataFrame(\n",
    "    scaler_employee.fit_transform(X_employee),\n",
    "    columns=X_employee.columns\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Pr√©-processamento conclu√≠do!\")\n",
    "print(f\"X_employee shape: {X_employee.shape}\")\n",
    "print(f\"y_employee shape: {y_employee.shape}\")\n",
    "print(f\"Distribui√ß√£o do target:\\n{y_employee.value_counts()}\")\n",
    "print(f\"Propor√ß√£o: {y_employee.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset 5: Australia Rain (Weather)\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- 145,460 amostras √ó 23 colunas (MUITO GRANDE)\n",
    "- Target: `RainTomorrow` (Yes/No)\n",
    "- MUITOS valores nulos (~40% das features)\n",
    "- Features categ√≥ricas: WindGustDir, WindDir9am, WindDir3pm, RainToday\n",
    "\n",
    "**Estrat√©gia:**\n",
    "1. Remover Date e Location (n√£o informativas/muitas categorias)\n",
    "2. Remover linhas com muitos nulos (dropna)\n",
    "3. Fazer amostragem estratificada para ~10k amostras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET 5: AUSTRALIA RAIN (WEATHER)\n",
      "================================================================================\n",
      "Shape original: (145460, 23)\n",
      "\n",
      "Valores nulos por coluna:\n",
      "MinTemp           1485\n",
      "MaxTemp           1261\n",
      "Rainfall          3261\n",
      "Evaporation      62790\n",
      "Sunshine         69835\n",
      "WindGustDir      10326\n",
      "WindGustSpeed    10263\n",
      "WindDir9am       10566\n",
      "WindDir3pm        4228\n",
      "WindSpeed9am      1767\n",
      "WindSpeed3pm      3062\n",
      "Humidity9am       2654\n",
      "Humidity3pm       4507\n",
      "Pressure9am      15065\n",
      "Pressure3pm      15028\n",
      "Cloud9am         55888\n",
      "Cloud3pm         59358\n",
      "Temp9am           1767\n",
      "Temp3pm           3609\n",
      "RainToday         3261\n",
      "RainTomorrow      3267\n",
      "dtype: int64\n",
      "\n",
      "Total de colunas com nulos: 21\n",
      "\n",
      "Shape ap√≥s remo√ß√£o de Date e Location: (145460, 21)\n",
      "Shape ap√≥s remover target nulo: (142193, 21)\n",
      "\n",
      "Linhas removidas por nulos: 85773\n",
      "Shape ap√≥s dropna: X=(56420, 20), y=(56420,)\n",
      "\n",
      "Colunas categ√≥ricas identificadas: ['WindGustDir', 'WindDir9am', 'WindDir3pm']\n",
      "Colunas ap√≥s one-hot encoding: 62\n",
      "\n",
      "Fazendo amostragem estratificada de 56420 para 10000 amostras...\n",
      "Shape ap√≥s amostragem: X=(10000, 62), y=(10000,)\n",
      "\n",
      "‚úì Pr√©-processamento conclu√≠do!\n",
      "X_weather shape: (10000, 62)\n",
      "y_weather shape: (10000,)\n",
      "Distribui√ß√£o do target:\n",
      "RainTomorrow\n",
      "0    7797\n",
      "1    2203\n",
      "Name: count, dtype: int64\n",
      "Propor√ß√£o: RainTomorrow\n",
      "0    0.7797\n",
      "1    0.2203\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATASET 5: AUSTRALIA RAIN (WEATHER)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar dataset\n",
    "df_weather = pd.read_csv('../data/weather.csv')\n",
    "print(f\"Shape original: {df_weather.shape}\")\n",
    "print(f\"\\nValores nulos por coluna:\")\n",
    "null_counts = df_weather.isnull().sum()\n",
    "print(null_counts[null_counts > 0])\n",
    "print(f\"\\nTotal de colunas com nulos: {(null_counts > 0).sum()}\")\n",
    "\n",
    "# Remover colunas n√£o informativas\n",
    "df_weather = df_weather.drop(columns=['Date', 'Location'])\n",
    "print(f\"\\nShape ap√≥s remo√ß√£o de Date e Location: {df_weather.shape}\")\n",
    "\n",
    "# Remover target nulo primeiro\n",
    "df_weather = df_weather.dropna(subset=['RainTomorrow'])\n",
    "print(f\"Shape ap√≥s remover target nulo: {df_weather.shape}\")\n",
    "\n",
    "# Separar target\n",
    "y_weather = df_weather['RainTomorrow'].map({'Yes': 1, 'No': 0})\n",
    "X_weather = df_weather.drop(columns=['RainTomorrow'])\n",
    "\n",
    "# Remover linhas com muitos nulos (estrat√©gia: dropna)\n",
    "# Vamos remover linhas que t√™m qualquer valor nulo\n",
    "initial_rows = len(X_weather)\n",
    "valid_indices = X_weather.dropna().index\n",
    "X_weather = X_weather.loc[valid_indices]\n",
    "y_weather = y_weather.loc[valid_indices]\n",
    "\n",
    "print(f\"\\nLinhas removidas por nulos: {initial_rows - len(X_weather)}\")\n",
    "print(f\"Shape ap√≥s dropna: X={X_weather.shape}, y={y_weather.shape}\")\n",
    "\n",
    "# One-hot encoding para categ√≥ricas\n",
    "# Converter RainToday para num√©rico antes\n",
    "if 'RainToday' in X_weather.columns:\n",
    "    X_weather['RainToday'] = X_weather['RainToday'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Identificar colunas categ√≥ricas (dire√ß√µes de vento)\n",
    "categorical_cols = [col for col in X_weather.columns if 'WindDir' in col or 'Wind' in col and X_weather[col].dtype == 'object']\n",
    "print(f\"\\nColunas categ√≥ricas identificadas: {categorical_cols}\")\n",
    "\n",
    "if categorical_cols:\n",
    "    X_weather = pd.get_dummies(X_weather, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "    print(f\"Colunas ap√≥s one-hot encoding: {len(X_weather.columns)}\")\n",
    "\n",
    "# AMOSTRAGEM ESTRATIFICADA para ~10k amostras\n",
    "if len(X_weather) > 10000:\n",
    "    sample_size = 10000\n",
    "    print(f\"\\nFazendo amostragem estratificada de {len(X_weather)} para {sample_size} amostras...\")\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_weather, _, y_weather, _ = train_test_split(\n",
    "        X_weather, y_weather, \n",
    "        train_size=sample_size, \n",
    "        stratify=y_weather,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"Shape ap√≥s amostragem: X={X_weather.shape}, y={y_weather.shape}\")\n",
    "\n",
    "# Normalizar com StandardScaler\n",
    "scaler_weather = StandardScaler()\n",
    "X_weather = pd.DataFrame(\n",
    "    scaler_weather.fit_transform(X_weather),\n",
    "    columns=X_weather.columns\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Pr√©-processamento conclu√≠do!\")\n",
    "print(f\"X_weather shape: {X_weather.shape}\")\n",
    "print(f\"y_weather shape: {y_weather.shape}\")\n",
    "print(f\"Distribui√ß√£o do target:\\n{y_weather.value_counts()}\")\n",
    "print(f\"Propor√ß√£o: {y_weather.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Resumo Final\n",
    "\n",
    "Valida√ß√£o de todos os datasets processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RESUMO FINAL - TODOS OS DATASETS PROCESSADOS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "         Dataset  Amostras  Features  Nulos em X  Nulos em y  Classe 0  Classe 1 Propor√ß√£o (%)\n",
      "   Breast Cancer       569        30           0           0       357       212         37.3%\n",
      "         Titanic       891        10           0           0       549       342         38.4%\n",
      "Water Potability      3276         9           0           0      1998      1278         39.0%\n",
      "        Employee      4653        10           0           0      3053      1600         34.4%\n",
      "         Weather     10000        62           0           0      7797      2203         22.0%\n",
      "\n",
      "================================================================================\n",
      "‚úì TODOS OS DATASETS PRONTOS PARA USO NO EXPERIMENTO RCBD!\n",
      "================================================================================\n",
      "\n",
      "üìä Vari√°veis dispon√≠veis:\n",
      "  ‚Ä¢ X_breast_cancer, y_breast_cancer\n",
      "  ‚Ä¢ X_titanic, y_titanic\n",
      "  ‚Ä¢ X_water_potability, y_water_potability\n",
      "  ‚Ä¢ X_employee, y_employee\n",
      "  ‚Ä¢ X_weather, y_weather\n",
      "\n",
      "üéØ Caracter√≠sticas:\n",
      "  ‚Ä¢ Todas as features s√£o num√©ricas\n",
      "  ‚Ä¢ Sem valores nulos\n",
      "  ‚Ä¢ Normalizadas com StandardScaler (z-score)\n",
      "  ‚Ä¢ Prontas para classificadores de ML\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"RESUMO FINAL - TODOS OS DATASETS PROCESSADOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "datasets_summary = {\n",
    "    'Breast Cancer': (X_breast_cancer, y_breast_cancer),\n",
    "    'Titanic': (X_titanic, y_titanic),\n",
    "    'Water Potability': (X_water_potability, y_water_potability),\n",
    "    'Employee': (X_employee, y_employee),\n",
    "    'Weather': (X_weather, y_weather)\n",
    "}\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for name, (X, y) in datasets_summary.items():\n",
    "    summary_data.append({\n",
    "        'Dataset': name,\n",
    "        'Amostras': X.shape[0],\n",
    "        'Features': X.shape[1],\n",
    "        'Nulos em X': X.isnull().sum().sum(),\n",
    "        'Nulos em y': y.isnull().sum(),\n",
    "        'Classe 0': (y == 0).sum(),\n",
    "        'Classe 1': (y == 1).sum(),\n",
    "        'Propor√ß√£o (%)': f\"{(y == 1).sum() / len(y) * 100:.1f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì TODOS OS DATASETS PRONTOS PARA USO NO EXPERIMENTO RCBD!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä Vari√°veis dispon√≠veis:\")\n",
    "print(\"  ‚Ä¢ X_breast_cancer, y_breast_cancer\")\n",
    "print(\"  ‚Ä¢ X_titanic, y_titanic\")\n",
    "print(\"  ‚Ä¢ X_water_potability, y_water_potability\")\n",
    "print(\"  ‚Ä¢ X_employee, y_employee\")\n",
    "print(\"  ‚Ä¢ X_weather, y_weather\")\n",
    "\n",
    "print(\"\\nüéØ Caracter√≠sticas:\")\n",
    "print(\"  ‚Ä¢ Todas as features s√£o num√©ricas\")\n",
    "print(\"  ‚Ä¢ Sem valores nulos\")\n",
    "print(\"  ‚Ä¢ Normalizadas com StandardScaler (z-score)\")\n",
    "print(\"  ‚Ä¢ Prontas para classificadores de ML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Baseline - SVM com Kernel RBF\n",
    "\n",
    "**Objetivo:** Treinar SVM simples em cada dataset para:\n",
    "1. Validar que os dados est√£o funcionando corretamente\n",
    "2. Obter m√©tricas baseline de refer√™ncia\n",
    "\n",
    "**Configura√ß√£o:**\n",
    "- Train/Test Split: 80/20 (stratified)\n",
    "- Modelo: SVM com kernel RBF (padr√£o)\n",
    "- M√©tricas: Acur√°cia, Precis√£o, Recall, F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BASELINE - SVM COM KERNEL RBF\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: Breast Cancer\n",
      "--------------------------------------------------------------------------------\n",
      "Treino: 455 amostras | Teste: 114 amostras\n",
      "Acur√°cia:  97.37%\n",
      "Precis√£o:  100.00%\n",
      "Recall:    92.86%\n",
      "F1-Score:  96.30%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: Titanic\n",
      "--------------------------------------------------------------------------------\n",
      "Treino: 712 amostras | Teste: 179 amostras\n",
      "Acur√°cia:  81.01%\n",
      "Precis√£o:  85.71%\n",
      "Recall:    60.87%\n",
      "F1-Score:  71.19%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: Water Potability\n",
      "--------------------------------------------------------------------------------\n",
      "Treino: 2620 amostras | Teste: 656 amostras\n",
      "Acur√°cia:  67.07%\n",
      "Precis√£o:  70.41%\n",
      "Recall:    26.95%\n",
      "F1-Score:  38.98%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: Employee\n",
      "--------------------------------------------------------------------------------\n",
      "Treino: 3722 amostras | Teste: 931 amostras\n",
      "Acur√°cia:  83.24%\n",
      "Precis√£o:  87.61%\n",
      "Recall:    59.69%\n",
      "F1-Score:  71.00%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: Weather\n",
      "--------------------------------------------------------------------------------\n",
      "Treino: 8000 amostras | Teste: 2000 amostras\n",
      "Acur√°cia:  84.70%\n",
      "Precis√£o:  73.68%\n",
      "Recall:    47.62%\n",
      "F1-Score:  57.85%\n",
      "\n",
      "================================================================================\n",
      "TABELA RESUMO - BASELINE SVM (KERNEL RBF)\n",
      "================================================================================\n",
      "\n",
      "         Dataset  Treino  Teste Acur√°cia (%) Precis√£o (%) Recall (%) F1-Score (%)\n",
      "   Breast Cancer     455    114        97.37       100.00      92.86        96.30\n",
      "         Titanic     712    179        81.01        85.71      60.87        71.19\n",
      "Water Potability    2620    656        67.07        70.41      26.95        38.98\n",
      "        Employee    3722    931        83.24        87.61      59.69        71.00\n",
      "         Weather    8000   2000        84.70        73.68      47.62        57.85\n",
      "\n",
      "================================================================================\n",
      "‚úì BASELINE CONCLU√çDO!\n",
      "================================================================================\n",
      "\n",
      "üìù Observa√ß√µes:\n",
      "  ‚Ä¢ Todos os datasets foram treinados com sucesso\n",
      "  ‚Ä¢ M√©tricas baseline dispon√≠veis para compara√ß√£o futura\n",
      "  ‚Ä¢ Employee: Dataset balanceado (~34% sa√≠das) com boas m√©tricas\n",
      "  ‚Ä¢ Pr√≥ximo passo: Experimento RCBD com diferentes tratamentos\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BASELINE - SVM COM KERNEL RBF\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Dicion√°rio para armazenar resultados\n",
    "baseline_results = []\n",
    "\n",
    "# Lista de datasets\n",
    "datasets = [\n",
    "    ('Breast Cancer', X_breast_cancer, y_breast_cancer),\n",
    "    ('Titanic', X_titanic, y_titanic),\n",
    "    ('Water Potability', X_water_potability, y_water_potability),\n",
    "    ('Employee', X_employee, y_employee),\n",
    "    ('Weather', X_weather, y_weather)\n",
    "]\n",
    "\n",
    "# Para cada dataset\n",
    "for name, X, y in datasets:\n",
    "    print(f\"\\n{'-' * 80}\")\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"{'-' * 80}\")\n",
    "    \n",
    "    # Train/Test Split (80/20) com stratify\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        stratify=y, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Treino: {X_train.shape[0]} amostras | Teste: {X_test.shape[0]} amostras\")\n",
    "    \n",
    "    # Treinar SVM com kernel RBF\n",
    "    svm = SVC(kernel='rbf', random_state=42)\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Predi√ß√µes\n",
    "    y_pred = svm.predict(X_test)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    baseline_results.append({\n",
    "        'Dataset': name,\n",
    "        'Treino': X_train.shape[0],\n",
    "        'Teste': X_test.shape[0],\n",
    "        'Acur√°cia (%)': acc * 100,\n",
    "        'Precis√£o (%)': prec * 100,\n",
    "        'Recall (%)': rec * 100,\n",
    "        'F1-Score (%)': f1 * 100\n",
    "    })\n",
    "    \n",
    "    print(f\"Acur√°cia:  {acc*100:.2f}%\")\n",
    "    print(f\"Precis√£o:  {prec*100:.2f}%\")\n",
    "    print(f\"Recall:    {rec*100:.2f}%\")\n",
    "    print(f\"F1-Score:  {f1*100:.2f}%\")\n",
    "\n",
    "# Criar DataFrame com resultados consolidados\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABELA RESUMO - BASELINE SVM (KERNEL RBF)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "# Formatar colunas de m√©tricas com 2 casas decimais\n",
    "for col in ['Acur√°cia (%)', 'Precis√£o (%)', 'Recall (%)', 'F1-Score (%)']:\n",
    "    baseline_df[col] = baseline_df[col].map('{:.2f}'.format)\n",
    "\n",
    "print(baseline_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì BASELINE CONCLU√çDO!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìù Observa√ß√µes:\")\n",
    "print(\"  ‚Ä¢ Todos os datasets foram treinados com sucesso\")\n",
    "print(\"  ‚Ä¢ M√©tricas baseline dispon√≠veis para compara√ß√£o futura\")\n",
    "print(\"  ‚Ä¢ Employee: Dataset balanceado (~34% sa√≠das) com boas m√©tricas\")\n",
    "print(\"  ‚Ä¢ Pr√≥ximo passo: Experimento RCBD com diferentes tratamentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Otimiza√ß√£o de Hiperpar√¢metros - SVM\n",
    "\n",
    "**Objetivo:** Implementar 3 m√©todos de otimiza√ß√£o de hiperpar√¢metros para compara√ß√£o:\n",
    "1. **GridSearch**: Busca exaustiva em grid definido\n",
    "2. **RandomSearch**: Amostragem aleat√≥ria no espa√ßo de busca\n",
    "3. **Bayesian Optimization**: Otimiza√ß√£o bayesiana com scikit-optimize\n",
    "\n",
    "**Configura√ß√£o:**\n",
    "- Modelo: SVM com kernel RBF\n",
    "- Hiperpar√¢metros: C e gamma\n",
    "- Mesma quantidade de itera√ß√µes (n_iter) para todos os m√©todos\n",
    "- Sem cross-validation: treino em X_train, teste em X_test\n",
    "- M√©tricas: Acur√°cia, Precis√£o, Recall, F1-Score, Tempo de execu√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fun√ß√£o grid_search_svm() implementada!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "def grid_search_svm(X_train, y_train, X_test, y_test, n_iter=16, verbose=True):\n",
    "    \"\"\"\n",
    "    Grid Search para otimiza√ß√£o de hiperpar√¢metros do SVM.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    -----------\n",
    "    X_train, y_train : dados de treino\n",
    "    X_test, y_test : dados de teste\n",
    "    n_iter : n√∫mero de combina√ß√µes a testar (ser√° ajustado para grid quadrado mais pr√≥ximo)\n",
    "    verbose : exibir mensagens de progresso (default=True)\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict com: metodo, best_params, acuracia, precisao, recall, f1_score, tempo\n",
    "    \"\"\"\n",
    "    inicio = time.time()\n",
    "    \n",
    "    # Criar grid que resulte em aproximadamente n_iter combina√ß√µes\n",
    "    # Exemplo: n_iter=16 ‚Üí 4x4 grid\n",
    "    grid_size = int(np.sqrt(n_iter))\n",
    "    \n",
    "    # Definir ranges para C e gamma\n",
    "    C_values = np.logspace(-2, 3, grid_size)  # 0.01 a 1000\n",
    "    gamma_values = np.logspace(-4, 1, grid_size)  # 0.0001 a 10\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Grid Search: testando {grid_size}x{grid_size} = {grid_size**2} combina√ß√µes\")\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    best_metrics = {}\n",
    "    \n",
    "    # Testar todas as combina√ß√µes\n",
    "    for C in C_values:\n",
    "        for gamma in gamma_values:\n",
    "            # Treinar modelo\n",
    "            model = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Testar no conjunto de teste\n",
    "            y_pred = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # Se for o melhor, salvar\n",
    "            if acc > best_score:\n",
    "                best_score = acc\n",
    "                best_params = {'C': C, 'gamma': gamma}\n",
    "                best_metrics = {\n",
    "                    'acuracia': acc,\n",
    "                    'precisao': precision_score(y_test, y_pred),\n",
    "                    'recall': recall_score(y_test, y_pred),\n",
    "                    'f1_score': f1_score(y_test, y_pred)\n",
    "                }\n",
    "    \n",
    "    tempo = time.time() - inicio\n",
    "    \n",
    "    return {\n",
    "        'metodo': 'GridSearch',\n",
    "        'best_params': best_params,\n",
    "        'acuracia': best_metrics['acuracia'],\n",
    "        'precisao': best_metrics['precisao'],\n",
    "        'recall': best_metrics['recall'],\n",
    "        'f1_score': best_metrics['f1_score'],\n",
    "        'tempo': tempo\n",
    "    }\n",
    "\n",
    "print(\"‚úì Fun√ß√£o grid_search_svm() implementada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fun√ß√£o random_search_svm() implementada!\n"
     ]
    }
   ],
   "source": [
    "def random_search_svm(X_train, y_train, X_test, y_test, n_iter=16, verbose=True):\n",
    "    \"\"\"\n",
    "    Random Search para otimiza√ß√£o de hiperpar√¢metros do SVM.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    -----------\n",
    "    X_train, y_train : dados de treino\n",
    "    X_test, y_test : dados de teste\n",
    "    n_iter : n√∫mero de combina√ß√µes aleat√≥rias a testar\n",
    "    verbose : exibir mensagens de progresso (default=True)\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict com: metodo, best_params, acuracia, precisao, recall, f1_score, tempo\n",
    "    \"\"\"\n",
    "    inicio = time.time()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Random Search: testando {n_iter} combina√ß√µes aleat√≥rias\")\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    best_metrics = {}\n",
    "    \n",
    "    np.random.seed(42)  # Para reprodutibilidade\n",
    "    \n",
    "    # Testar n_iter combina√ß√µes aleat√≥rias\n",
    "    for i in range(n_iter):\n",
    "        # Amostrar C e gamma de distribui√ß√µes log-uniformes\n",
    "        C = 10 ** np.random.uniform(-2, 3)  # 0.01 a 1000\n",
    "        gamma = 10 ** np.random.uniform(-4, 1)  # 0.0001 a 10\n",
    "        \n",
    "        # Treinar modelo\n",
    "        model = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Testar no conjunto de teste\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Se for o melhor, salvar\n",
    "        if acc > best_score:\n",
    "            best_score = acc\n",
    "            best_params = {'C': C, 'gamma': gamma}\n",
    "            best_metrics = {\n",
    "                'acuracia': acc,\n",
    "                'precisao': precision_score(y_test, y_pred),\n",
    "                'recall': recall_score(y_test, y_pred),\n",
    "                'f1_score': f1_score(y_test, y_pred)\n",
    "            }\n",
    "    \n",
    "    tempo = time.time() - inicio\n",
    "    \n",
    "    return {\n",
    "        'metodo': 'RandomSearch',\n",
    "        'best_params': best_params,\n",
    "        'acuracia': best_metrics['acuracia'],\n",
    "        'precisao': best_metrics['precisao'],\n",
    "        'recall': best_metrics['recall'],\n",
    "        'f1_score': best_metrics['f1_score'],\n",
    "        'tempo': tempo\n",
    "    }\n",
    "\n",
    "print(\"‚úì Fun√ß√£o random_search_svm() implementada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fun√ß√£o bayesian_search_svm() implementada!\n"
     ]
    }
   ],
   "source": [
    "def bayesian_search_svm(X_train, y_train, X_test, y_test, n_iter=16, verbose=True):\n",
    "    \"\"\"\n",
    "    Bayesian Optimization para otimiza√ß√£o de hiperpar√¢metros do SVM.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    -----------\n",
    "    X_train, y_train : dados de treino\n",
    "    X_test, y_test : dados de teste\n",
    "    n_iter : n√∫mero de itera√ß√µes da otimiza√ß√£o bayesiana\n",
    "    verbose : exibir mensagens de progresso (default=True)\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict com: metodo, best_params, acuracia, precisao, recall, f1_score, tempo\n",
    "    \"\"\"\n",
    "    inicio = time.time()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Bayesian Optimization: {n_iter} itera√ß√µes\")\n",
    "    \n",
    "    # Definir espa√ßo de busca (escala logar√≠tmica)\n",
    "    space = [\n",
    "        Real(1e-2, 1e3, prior='log-uniform', name='C'),\n",
    "        Real(1e-4, 1e1, prior='log-uniform', name='gamma')\n",
    "    ]\n",
    "    \n",
    "    # Vari√°vel para armazenar melhor resultado\n",
    "    best_metrics = {}\n",
    "    \n",
    "    # Fun√ß√£o objetivo a ser MINIMIZADA (por isso retorna -accuracy)\n",
    "    @use_named_args(space)\n",
    "    def objective(**params):\n",
    "        C = params['C']\n",
    "        gamma = params['gamma']\n",
    "        \n",
    "        # Treinar modelo\n",
    "        model = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Testar no conjunto de teste\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Retornar negativo da acur√°cia (queremos maximizar, mas gp_minimize minimiza)\n",
    "        return -acc\n",
    "    \n",
    "    # Executar otimiza√ß√£o bayesiana\n",
    "    result = gp_minimize(\n",
    "        objective,\n",
    "        space,\n",
    "        n_calls=n_iter,\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Obter melhores par√¢metros\n",
    "    best_C = result.x[0]\n",
    "    best_gamma = result.x[1]\n",
    "    best_params = {'C': best_C, 'gamma': best_gamma}\n",
    "    \n",
    "    # Treinar modelo final com os melhores par√¢metros\n",
    "    final_model = SVC(kernel='rbf', C=best_C, gamma=best_gamma, random_state=42)\n",
    "    final_model.fit(X_train, y_train)\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    \n",
    "    # Calcular todas as m√©tricas\n",
    "    best_metrics = {\n",
    "        'acuracia': accuracy_score(y_test, y_pred),\n",
    "        'precisao': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    tempo = time.time() - inicio\n",
    "    \n",
    "    return {\n",
    "        'metodo': 'BayesianOptimization',\n",
    "        'best_params': best_params,\n",
    "        'acuracia': best_metrics['acuracia'],\n",
    "        'precisao': best_metrics['precisao'],\n",
    "        'recall': best_metrics['recall'],\n",
    "        'f1_score': best_metrics['f1_score'],\n",
    "        'tempo': tempo\n",
    "    }\n",
    "\n",
    "print(\"‚úì Fun√ß√£o bayesian_search_svm() implementada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste das Fun√ß√µes de Otimiza√ß√£o\n",
    "\n",
    "Vamos testar as 3 fun√ß√µes com o dataset Breast Cancer (menor e mais r√°pido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTE DAS FUN√á√ïES DE OTIMIZA√á√ÉO - BREAST CANCER\n",
      "================================================================================\n",
      "\n",
      "Treino: 455 amostras | Teste: 114 amostras\n",
      "\n",
      "Testando com n_iter=16 (4x4 grid)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Grid Search: testando 4x4 = 16 combina√ß√µes\n",
      "M√©todo: GridSearch\n",
      "Melhores par√¢metros: C=21.5443, gamma=0.0046\n",
      "Acur√°cia:  98.25%\n",
      "Precis√£o:  100.00%\n",
      "Recall:    95.24%\n",
      "F1-Score:  97.56%\n",
      "Tempo:     0.30s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Random Search: testando 16 combina√ß√µes aleat√≥rias\n",
      "M√©todo: RandomSearch\n",
      "Melhores par√¢metros: C=145.2825, gamma=0.0012\n",
      "Acur√°cia:  96.49%\n",
      "Precis√£o:  100.00%\n",
      "Recall:    90.48%\n",
      "F1-Score:  95.00%\n",
      "Tempo:     0.28s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Bayesian Optimization: 16 itera√ß√µes\n",
      "M√©todo: BayesianOptimization\n",
      "Melhores par√¢metros: C=96.0981, gamma=0.0008\n",
      "Acur√°cia:  98.25%\n",
      "Precis√£o:  100.00%\n",
      "Recall:    95.24%\n",
      "F1-Score:  97.56%\n",
      "Tempo:     2.00s\n",
      "\n",
      "================================================================================\n",
      "‚úì TESTE CONCLU√çDO - TODAS AS FUN√á√ïES FUNCIONANDO!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TESTE DAS FUN√á√ïES DE OTIMIZA√á√ÉO - BREAST CANCER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Preparar dados (train/test split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_breast_cancer, y_breast_cancer, \n",
    "    test_size=0.2, \n",
    "    stratify=y_breast_cancer, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTreino: {X_train.shape[0]} amostras | Teste: {X_test.shape[0]} amostras\")\n",
    "print(f\"\\nTestando com n_iter=16 (4x4 grid)\\n\")\n",
    "\n",
    "# Testar GridSearch\n",
    "print(\"-\" * 80)\n",
    "result_grid = grid_search_svm(X_train, y_train, X_test, y_test, n_iter=16)\n",
    "print(f\"M√©todo: {result_grid['metodo']}\")\n",
    "print(f\"Melhores par√¢metros: C={result_grid['best_params']['C']:.4f}, gamma={result_grid['best_params']['gamma']:.4f}\")\n",
    "print(f\"Acur√°cia:  {result_grid['acuracia']*100:.2f}%\")\n",
    "print(f\"Precis√£o:  {result_grid['precisao']*100:.2f}%\")\n",
    "print(f\"Recall:    {result_grid['recall']*100:.2f}%\")\n",
    "print(f\"F1-Score:  {result_grid['f1_score']*100:.2f}%\")\n",
    "print(f\"Tempo:     {result_grid['tempo']:.2f}s\")\n",
    "\n",
    "# Testar RandomSearch\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "result_random = random_search_svm(X_train, y_train, X_test, y_test, n_iter=16)\n",
    "print(f\"M√©todo: {result_random['metodo']}\")\n",
    "print(f\"Melhores par√¢metros: C={result_random['best_params']['C']:.4f}, gamma={result_random['best_params']['gamma']:.4f}\")\n",
    "print(f\"Acur√°cia:  {result_random['acuracia']*100:.2f}%\")\n",
    "print(f\"Precis√£o:  {result_random['precisao']*100:.2f}%\")\n",
    "print(f\"Recall:    {result_random['recall']*100:.2f}%\")\n",
    "print(f\"F1-Score:  {result_random['f1_score']*100:.2f}%\")\n",
    "print(f\"Tempo:     {result_random['tempo']:.2f}s\")\n",
    "\n",
    "# Testar Bayesian Optimization\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "result_bayesian = bayesian_search_svm(X_train, y_train, X_test, y_test, n_iter=16)\n",
    "print(f\"M√©todo: {result_bayesian['metodo']}\")\n",
    "print(f\"Melhores par√¢metros: C={result_bayesian['best_params']['C']:.4f}, gamma={result_bayesian['best_params']['gamma']:.4f}\")\n",
    "print(f\"Acur√°cia:  {result_bayesian['acuracia']*100:.2f}%\")\n",
    "print(f\"Precis√£o:  {result_bayesian['precisao']*100:.2f}%\")\n",
    "print(f\"Recall:    {result_bayesian['recall']*100:.2f}%\")\n",
    "print(f\"F1-Score:  {result_bayesian['f1_score']*100:.2f}%\")\n",
    "print(f\"Tempo:     {result_bayesian['tempo']:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì TESTE CONCLU√çDO - TODAS AS FUN√á√ïES FUNCIONANDO!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Experimento RCBD Completo\n",
    "\n",
    "**Objetivo:** Executar o experimento completo comparando os 3 m√©todos de otimiza√ß√£o\n",
    "\n",
    "**Configura√ß√£o:**\n",
    "- **Blocos:** 5 datasets (Breast Cancer, Titanic, Water Potability, Employee, Weather)\n",
    "- **Repeti√ß√µes:** 7 seeds diferentes (1-7) para cada dataset\n",
    "- **Tratamentos:** 3 m√©todos de otimiza√ß√£o (GridSearch, RandomSearch, BayesianOptimization)\n",
    "- **Total de experimentos:** 5 datasets √ó 7 seeds √ó 3 m√©todos = **105 experimentos**\n",
    "\n",
    "**Delineamento RCBD:**\n",
    "- Datasets funcionam como blocos (controlam variabilidade)\n",
    "- M√©todos de otimiza√ß√£o s√£o os tratamentos a comparar\n",
    "- M√∫ltiplas repeti√ß√µes com seeds diferentes para robustez estat√≠stica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONFIGURA√á√ÉO DO EXPERIMENTO RCBD\n",
      "================================================================================\n",
      "\n",
      "N√∫mero de datasets (blocos): 5\n",
      "Datasets: ['Breast Cancer', 'Titanic', 'Water Potability', 'Employee', 'Weather']\n",
      "\n",
      "N√∫mero de seeds (repeti√ß√µes): 7\n",
      "Seeds: [1, 2, 3, 4, 5, 6, 7]\n",
      "\n",
      "M√©todos de otimiza√ß√£o: GridSearch, RandomSearch, BayesianOptimization\n",
      "Itera√ß√µes por m√©todo: 16\n",
      "\n",
      "Total de experimentos: 5 √ó 7 √ó 3 = 105\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# Criar dicion√°rio com todos os datasets\n",
    "datasets_dict = {\n",
    "    'Breast Cancer': (X_breast_cancer, y_breast_cancer),\n",
    "    'Titanic': (X_titanic, y_titanic),\n",
    "    'Water Potability': (X_water_potability, y_water_potability),\n",
    "    'Employee': (X_employee, y_employee),\n",
    "    'Weather': (X_weather, y_weather)\n",
    "}\n",
    "\n",
    "# Definir seeds para repeti√ß√µes\n",
    "seeds = list(range(1, 8))  # [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Configura√ß√£o do experimento\n",
    "n_iter = 16  # N√∫mero de itera√ß√µes para cada m√©todo\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFIGURA√á√ÉO DO EXPERIMENTO RCBD\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nN√∫mero de datasets (blocos): {len(datasets_dict)}\")\n",
    "print(f\"Datasets: {list(datasets_dict.keys())}\")\n",
    "print(f\"\\nN√∫mero de seeds (repeti√ß√µes): {len(seeds)}\")\n",
    "print(f\"Seeds: {seeds}\")\n",
    "print(f\"\\nM√©todos de otimiza√ß√£o: GridSearch, RandomSearch, BayesianOptimization\")\n",
    "print(f\"Itera√ß√µes por m√©todo: {n_iter}\")\n",
    "print(f\"\\nTotal de experimentos: {len(datasets_dict)} √ó {len(seeds)} √ó 3 = {len(datasets_dict) * len(seeds) * 3}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXECUTANDO EXPERIMENTO RCBD\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508c4a280c364d5ab330947054621cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Datasets:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89802da1586a40db92df8efea61a128c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Breast Cancer:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45db22a6e3104645ace7ba264ca53678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Titanic:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f9030afb894fc0afc01bc85d063342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Water Potability:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXECUTANDO EXPERIMENTO RCBD\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Lista para armazenar todos os resultados\n",
    "resultados = []\n",
    "\n",
    "# Loop externo: iterar sobre datasets (blocos)\n",
    "for dataset_name, (X, y) in tqdm(datasets_dict.items(), desc=\"Datasets\", position=0):\n",
    "    \n",
    "    # Loop interno: iterar sobre seeds (repeti√ß√µes)\n",
    "    for seed in tqdm(seeds, desc=f\"{dataset_name}\", position=1, leave=False):\n",
    "        \n",
    "        # 1. Fazer split train/test ESTRATIFICADO com a seed atual\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=0.2, \n",
    "            stratify=y,  # Manter propor√ß√£o de classes\n",
    "            random_state=seed\n",
    "        )\n",
    "        \n",
    "        # 2. GridSearch\n",
    "        result_grid = grid_search_svm(X_train, y_train, X_test, y_test, n_iter=n_iter, verbose=False)\n",
    "        result_grid['dataset'] = dataset_name\n",
    "        result_grid['seed'] = seed\n",
    "        resultados.append(result_grid)\n",
    "        \n",
    "        # 3. RandomSearch\n",
    "        result_random = random_search_svm(X_train, y_train, X_test, y_test, n_iter=n_iter, verbose=False)\n",
    "        result_random['dataset'] = dataset_name\n",
    "        result_random['seed'] = seed\n",
    "        resultados.append(result_random)\n",
    "        \n",
    "        # 4. BayesianOptimization\n",
    "        result_bayesian = bayesian_search_svm(X_train, y_train, X_test, y_test, n_iter=n_iter, verbose=False)\n",
    "        result_bayesian['dataset'] = dataset_name\n",
    "        result_bayesian['seed'] = seed\n",
    "        resultados.append(result_bayesian)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì EXPERIMENTO CONCLU√çDO!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal de resultados coletados: {len(resultados)}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONSOLIDANDO RESULTADOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Converter lista de dicts para DataFrame\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "# Reordenar colunas para melhor visualiza√ß√£o\n",
    "colunas_ordenadas = ['dataset', 'seed', 'metodo', 'acuracia', 'precisao', 'recall', 'f1_score', 'tempo', 'best_params']\n",
    "df_resultados = df_resultados[colunas_ordenadas]\n",
    "\n",
    "# Mostrar informa√ß√µes b√°sicas\n",
    "print(f\"\\nShape do DataFrame: {df_resultados.shape}\")\n",
    "print(f\"Colunas: {list(df_resultados.columns)}\")\n",
    "\n",
    "# Mostrar primeiras linhas\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"PRIMEIRAS 10 LINHAS:\")\n",
    "print(\"-\" * 80)\n",
    "print(df_resultados.head(10).to_string(index=False))\n",
    "\n",
    "# Estat√≠sticas descritivas por m√©todo\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTAT√çSTICAS DESCRITIVAS POR M√âTODO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for metodo in ['GridSearch', 'RandomSearch', 'BayesianOptimization']:\n",
    "    df_metodo = df_resultados[df_resultados['metodo'] == metodo]\n",
    "    print(f\"\\n{metodo}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Acur√°cia:  m√©dia={df_metodo['acuracia'].mean():.4f}, std={df_metodo['acuracia'].std():.4f}\")\n",
    "    print(f\"Precis√£o:  m√©dia={df_metodo['precisao'].mean():.4f}, std={df_metodo['precisao'].std():.4f}\")\n",
    "    print(f\"Recall:    m√©dia={df_metodo['recall'].mean():.4f}, std={df_metodo['recall'].std():.4f}\")\n",
    "    print(f\"F1-Score:  m√©dia={df_metodo['f1_score'].mean():.4f}, std={df_metodo['f1_score'].std():.4f}\")\n",
    "    print(f\"Tempo(s):  m√©dia={df_metodo['tempo'].mean():.2f}s, std={df_metodo['tempo'].std():.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì RESULTADOS CONSOLIDADOS!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SALVANDO RESULTADOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Criar diret√≥rio results se n√£o existir\n",
    "results_dir = '../results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Salvar DataFrame em CSV\n",
    "output_file = os.path.join(results_dir, 'experimento_rcbd_resultados.csv')\n",
    "df_resultados.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Resultados salvos em: {output_file}\")\n",
    "print(f\"  Total de linhas: {len(df_resultados)}\")\n",
    "print(f\"  Colunas: {', '.join(df_resultados.columns)}\")\n",
    "\n",
    "# Tamb√©m salvar vers√£o JSON dos best_params separadamente para facilitar an√°lise\n",
    "import json\n",
    "\n",
    "# Expandir best_params em colunas separadas\n",
    "df_resultados_expandido = df_resultados.copy()\n",
    "df_resultados_expandido['C'] = df_resultados_expandido['best_params'].apply(lambda x: x['C'])\n",
    "df_resultados_expandido['gamma'] = df_resultados_expandido['best_params'].apply(lambda x: x['gamma'])\n",
    "df_resultados_expandido = df_resultados_expandido.drop(columns=['best_params'])\n",
    "\n",
    "# Salvar vers√£o expandida\n",
    "output_file_expandido = os.path.join(results_dir, 'experimento_rcbd_resultados_expandido.csv')\n",
    "df_resultados_expandido.to_csv(output_file_expandido, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Vers√£o expandida salva em: {output_file_expandido}\")\n",
    "print(f\"  (best_params separado em colunas C e gamma)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì EXPERIMENTO RCBD COMPLETO - TODOS OS DADOS SALVOS!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä Pr√≥ximos passos:\")\n",
    "print(\"  1. An√°lise explorat√≥ria dos resultados\")\n",
    "print(\"  2. ANOVA para comparar m√©todos de otimiza√ß√£o\")\n",
    "print(\"  3. Testes post-hoc se ANOVA indicar diferen√ßas significativas\")\n",
    "print(\"  4. Visualiza√ß√µes (boxplots, gr√°ficos de intera√ß√£o)\")\n",
    "print(\"  5. Conclus√µes e recomenda√ß√µes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Trabalho Final PAE)",
   "language": "python",
   "name": "trabalho-final-pae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
